**📌 IPC 및 스레드 안전성**

**OS-064**

IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요.

- 하나의 컴퓨터에서 실행중인 서로 다른 프로세스 간에 발생하는 통신
    - 메세지 전달 방식
        - 서로 독립된 공간에 있는 프로세스들이 자원을 공유하기 위해 커널에 있는 공유 공간 속에 데이터를 주고 받는 방식
    - 공유 메모리 방식
        - 프로세스들이 주소 공간의 일부를 공유하는 방식

**OS-065**

Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.

- 여러 장치(CPU)나 여러 프로세스가 공동으로 사용하는 메모리

**OS-066**

메시지 큐는 단방향이라고 할 수 있나요?

- 메세지 큐는 데이터를 전송하는 생산자와 데이터를 받아서 처리하는 소비자로 역할을 명확하게 분리함
- 그러므로 단방향임

**OS-067**

Thread Safe 하다는 것은 어떤 의미인가요?

- 여러 개의 스레드가 동시에 접근하여 특정 코드, 데이터 구조 또는 리소스를 사용하더라도 경쟁 조건이 발생하지 않고, 프로그램 결과가 항상 올바르고 예측 가능함을 의미

**OS-068**

Thread Safe 를 보장하기 위해 어떤 방법을 사용할 수 있나요?

- 뮤텍스
    - 해당 자원이 사용 불가인 경우 Lock을 걸고, 사용이 끝나면 Lock을 해제
- 세마포어
    - 자원에 접근할 수 있는 카운터를 설정하여 동시 접근을 제어

**OS-069**

Peterson's Algorithm 이 무엇이며, 한계점에 대해 설명해 주세요.

- 두 개의 스레드 또는 프로세스가 임계 구역에 동시에 진입하는 것을 방지하여 상호 배제를 보장하는 소프트웨어 기반의 동기화 알고리즘
- 한계
    - 2개의 스레드 or 프로세스밖에 제어하지 못 함. 현재는 사용을 할 수가 없음
    - 현대의 컴파일러와 CPU는 성능 향상을 위해 명령어 재배치를 수행함. 즉, 코드에서 작성된 순서대로 명령어가 실행되는 것이 아니라, 메모리 접근 순서가 바뀔 수 있음. → 스레드가 잘못된 상태를 보고 임계 구역에 동시에 진입할 수 있음

**OS-070**

Race Condition 이 무엇인가요?

**OS-071**

Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?

1. 불변 객체
    - 공유 자원의 상태를 수정 불가능하게 만드는 것
2. 스레드 로컬 저장소
    - 공유 자원을 사용하지 않고, 각 스레드가 자신만의 독립된 복사본을 사용하도록 하는 방식
3. 원자적 연산
    - 락킹 대신 하드웨어 수준의 명령어(CPU 명령어)를 사용하여 변수 업데이트의 원자성을 보장하는 방법
4. 동시성 컬렉션
    - 멀티스레드 환경을 위해 특수한 자료구조를 사용

**OS-072**

Thread Pool, Monitor, Fork-Join에 대해 설명해 주세요.

1. 스레드 풀
    - 미리 생성된 스레드들을 모아 관리하는 기술로 스레드를 생성하고 소멸시키는 오버헤드를 줄여 시스템 성능을 최적화함
2. 모니터
    - 임계 구역을 안전하게 보호하고 스레드 간의 통신을 관리하기 위해 설계된 프로그래밍 언어 수준의 구조체
3. 포크-조인 프레임워크
    - 대규모 병렬 작업을 효율적으로 처리하기 위해 **분할 정복(Divide and Conquer)** 알고리즘을 구현한 특수한 스레드 풀 기반의 프레임워크

**OS-073**

Thread Pool을 사용한다고 가정하면, 어떤 기준으로 스레드의 수를 결정할 것인가요?

1. 처리하려는 작업의 성격
2. 시스템의 CPU 코어 수

**OS-074**

어떤 데이터를 정렬 하려고 합니다. 어떤 방식의 전략을 사용하는 것이 가장 안전하면서도 좋은 성능을 낼 수 있을까요?

- 머지소트. 최악의 상황에도 가장 좋은 성능을 유지하기 때문에

---

**📌 캐시**

**OS-075**

캐시 메모리 및 메모리 계층성에 대해 설명해 주세요.

- CPU가 RAM에 접근할 때 발생하는 속도 차이를 줄여 시스템 전체 성능을 높이는 임시 기억 장치
- 성능 향상을 위해 서로 다른 속도, 용량, 가격을 가진 여러 장치들을 계층적으로 사용
    - CPU에서 가까울 수록 빠르고(고가) 용량이 작아지며, 멀어질수록 느리고(저가) 용량이 커지는 피라미드 형태를 이룸

**OS-076**

캐시 메모리는 어디에 위치해 있나요?

- 캐시 메모리는 주로 CPU 내부와 주기억장치(RAM) 사이에 위치함

**OS-077**

L1, L2 캐시에 대해 설명해 주세요.

- L1 캐시
    - CPU 코어 내부
    - 가장 빠름
    - 용량은 가장 작음
    - 코어 전용
- L2 캐시
    - L1보다는 멀지만, CPU 칩 내부에 위치
    - 속도는 L1보다 느림
    - 용량은 L1보다 큼
    - 코어마다 전용으로 두거나, 코어 그룹끼리 공유

**OS-078**

캐시에 올라오는 데이터는 어떻게 관리되나요?

- 매핑 방식, 교체 정책, 쓰기 정책으로 관리
    - 매핑 방식 : RAM의 특정 주소에 있는 데이터를 캐시 메모리의 어느 위치에 저장할지 결정하는 방식
        - 직접 매핑
        - 완전 연관 매핑
        - 집합 연관 매핑
    - 교체 정책 : 캐시가 가득 찼을 때, 새로 들어오는 데이터를 위해 어떤 캐시 라인을 비울지 결정하는 규칙
        - LRU
        - FIFO
        - LFU
        - RANDOM
    - 쓰기 정책 : CPU가 캐시의 데이터를 변경했을 때, 그 변경 내용을 주 기억장치(RAM)에 언제 반영할지 결정하는 정책
        - 쓰기 즉시 반영
        - 쓰기 지연 반영

**OS-079**

캐시간의 동기화는 어떻게 이루어지나요?

- 캐시 일관성 유지를 위해 두 가지 기본 프로토콜이 있음
    1. 스누핑 프로토콜
        - 공유 버스를 통해 통신하는 시스템에서 주로 사용되는 방식
    2. 디렉토리 기반 프로토콜
        - 대규모 멀티프로세서 시스템이나 분산 메모리 환경에서 버스의 한계를 극복하기 위해 사용됨
    3. MESI 프로토콜
        - 실제 CPU(인텔, AMD)에서 캐시 일관성을 유지하기 위해 가장 널리 사용되는 상태 기반 스누핑 프로토콜

**OS-080**

캐시 메모리의 Mapping 방식에 대해 설명해 주세요.

- 직접 매핑
    - RAM의 주소를 캐시의 주소와 직접 연결함
- 완전 연관 매핑
    - RAM의 특정 주소는 캐시 메모리 내의 모든 위치에 자유롭게 저장 가능
- 집합 연관 매핑
    - 직접 매핑과 완전 연관 매핑의 장점을 결합한 절충안

**OS-081**

캐시의 지역성에 대해 설명해 주세요.

- 컴퓨터 프로그램이 실행될 때 메모리의 특정 부분에 집중적으로 접근하는 경향을 의미
    1. 시간적 지역성
        - 최근에 접근했던 데이터는 가까운 미래에 다시 접근될 가능성이 높음
    2. 공간적 지역성
        - 접근한 데이터 근처의 메모리 위치에 있는 데이터는 곧 접근될 가능성이 높다는 원리

**OS-082**

캐시의 지역성을 기반으로, 이차원 배열을 가로/세로로 탐색했을 때의 성능 차이에 대해 설명해 주세요.

- 대부분의 프로그래밍 언어와 시스템 환경에서 가로(행) 단위 탐색이 세로(열) 단위 탐색보다 훨씬 빠름

**OS-083**

캐시의 공간 지역성은 어떻게 구현될 수 있을까요? (힌트: 캐시는 어떤 단위로 저장되고 관리될까요?

- 하나의 데이터를 가져오면, 그 주변 데이터도 함께 가져온다.
1. 캐시 라인 단위 저장 및 관리
    - 캐시가 데이터를 캐시 라인이라는 블록 단위로 관리하는 것
2. 매핑 방식
- 캐시 라인을 캐시 메모리의 어느 위치에 배치하느냐에 따라 공간적 지역성의 효율성이 달라짐. 특히 **집합 연관 매핑(Set-Associative Mapping)** 방식이 공간적 지역성 활용에 유리함
    - **집합 연관 매핑:** RAM의 데이터가 캐시의 특정 **집합(Set)** 내의 여러 위치 중 하나에 저장될 수 있게 함. 이는 충돌을 줄여주기 때문에, 여러 개의 인접한 데이터 블록(캐시 라인)이 서로를 덮어쓰지 않고 캐시에 공존할 가능성이 높아짐
        - 예를 들어, 배열을 순차적으로 탐색할 때 여러 캐시 라인이 캐시에 안정적으로 머무를 수 있게 하여 공간적 지역성을 극대화함.

---

**📌 메모리 할당**

**OS-084**

연속할당 방식 세 가지를 설명해주세요. (first-fit, best-fit, worst-fit)

- 연속할당 : 주기억장치의 공간을 프로세스에 할당하는 방법 중 하나로, 프로세스의 전체 영역이 메모리 내의 연속된 하나의 블록에 할당되는 방식
1. 최적 적합
    - 메모리의 모든 빈 공간을 탐색하여 요청된 크기보다 크거나 같은 크기 중 **차이가 최소인** 공간을 선택함
2. 최초 적합
    - 메모리의 시작 또는 마지막부터 순차적으로 탐색하며, N보다 크거나 같은 크기의 첫 번째 빈 공간을 찾으면 바로 할당
3. 최악 적합
    - 메모리의 모든 빈 공간을 탐색하여 요청된 크기보다 크거나 같은 크기 중 차이가  최대인 공간을 선택

**OS-085**

worst-fit 은 언제 사용할 수 있을까요?

- worst-fit의 주요 목적은 할당 후 남은 조각의 크기를 최대화하는 것
    - 동적 분할이 활발하지 않은 환경 (프로세스의 메모리 요구가 매우 큰 경우)
    - 메모리 통합이 자주 발생하는 환경

**OS-086**

성능이 가장 좋은 알고리즘은 무엇일까요?

- First-Fit - 요청 크기를 만족하는 첫 번째 공간을 무조건 할당하니까 가장 빠름

**OS-087**

Thrashing 이란 무엇인가요?

- 컴퓨터 시스템에서 프로세스가 작업을 수행하는 시간보다 페이지 교체에 시간을 훨씬 더 많이 소비하여, CPU 이용률이 급격히 떨어지고 시스템의 전반적인 성능이 저하되는 현상
1. 발생 원인 : 과도한 다중 프로그래밍과 페이지 부재
2. 악순환
    1. CPU 이용률 감소
    2. 스케줄러의 오해
    3. 상황 악화
    4. 성능 붕괴
3. 해결 전략
    - 작업 집합 모델 : 프로세스가 일정 시간 동안 집중적으로 접근하는 페이지들의 집합을 계산하고, 이 작업 집합 전체를 메모리에 할당
    - 페이지 부재 빈도 제어 : 프로세스의 페이지 부재 빈도가 너무 높아지면 더 많은 프레임을 할당, 너무 낮아지면 프레임을 회수하여 적정 수준 유지
    - 다중 프로그래밍 정도 조절 : 시스템 전체의 메모리 할당량을 모니터링

**OS-088**

Thrashing 발생 시, 어떻게 완화할 수 있을까요?

- 87-3에서 설명

---

**📌 가상 메모리**

**OS-089**

가상 메모리란 무엇인가요?

- 운영체제가 제공하는 핵심 기술 중 하나로, RAM 용량보다 훨씬 큰 메모리 공간을 사용자 프로그램에게 제공하고 관리하는 기법임

**OS-090**

가상 메모리가 가능한 이유가 무엇일까요?

- 지역성 특성을 이용한 페이징 기법, 하드웨어의 지원
    - 운영체제는 프로그램 전체를 RAM에 올릴 필요 없이, 현재 실행에 필요한 최소한의 페이지만 RAM에 올려놓음. 나머지 사용되지 않는 페이지들은 보조 기억장치에 보관
    - MMU - CPU와 RAM 사이에 위치한 하드웨어 장치. 프로그램이 요청하는 가상 주소를 받아 실제 물리 주소로 변환하는 작업을 하드웨어로 매우 빠르게 처리
    - 페이지 테이블 - 가상 페이지 번호가 실제 RAM의 어느 페이지 프레임에 저장되어 있는지 기록
- 페이징 기법
    - 불연속 할당 : 프로그램의 논리적 메모리를 페이지라는 고정된 크기로 나누고, 실제 메모리(RAM)를 페이지 프레임이라는 같은 크기의 블록으로 나눔
    - 단편화 해결 : 프로그램의 페이지들이 RAM의 불연속적인 위치에 자유롭게 배치될 수 있어, 연속 할당 방식에서 발생하는 외부 단편화 문제를 효과적으로 해결

**OS-091**

Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.

- 프로세스의 실행을 중단하고 디스크에서 필요한 페이지를 RAM으로 가져오는 과정을 수행
1. 페이지 부재 발생 및 감지
2. 페이지 부재 처리 루틴 실행 (OS)
3. 프로세스 재개

**OS-092**

페이지 크기에 대한 Trade-Off를 설명해 주세요.

- 페이지 크기가 작을 때
    - 장점 - 내부 단편화 감소, 지역성 활용 최적화 가능
    - 단점 - 페이지 테이블 크기 증가, 디스크 I/O 횟수 증가, 오버헤드 증가
- 페이지 크기가 클 때
    - 장점 - 페이지 테이블 크기 감소, 디스크 I/O 효율 증 , TLB 효율 증가
    - 단점 - 내부 단편화 심화, 스레싱 위험 증가

**OS-093**

페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?

- 페이지 크기가 커지면 오히려 페이지 부재 발생 빈도는 줄어드는 경향이 있음
    - 공간적 지역성 때문
- 하지만 특정 임계점 이상 커진다면?
    - 내부 단편화가 증가
    - 작업 집합 효율이 저하됨

**OS-094**

세그멘테이션 방식을 사용하고 있다면, 가상 메모리를 사용할 수 없을까요?

- 가상 메모리는 프로세스에게 RAM보다 큰 논리적 주소 공간을 제공하는 개념 그 자체를 의미, 세그멘테이션은 이를 구현하는 방식 중 하나

**OS-095**

세그멘테이션과 페이징의 차이점은 무엇인가요?

- 메모리를 나누는 방식과 주소의 논리적 의미에서 근본적인 차이가 있음
- 페이지
    - 분할 단위가 페이지와 페이지 프레임
    - 고정 크기
    - 내부 단편화만 발생
- 세그멘테이션
    - 분할 단위가 세그먼트 단위
    - 가변 크기
    - 외부 단편화가 발생

**OS-096**

페이지와 프레임의 차이에 대해 설명해 주세요.

- 페이지
    - 프로세스가 인식하는 논리적 주소 공간을 일정한 크기로 나눈 단위를 의미
        - 위치: 프로세스의 가상 메모리 주소 공간에 존재
        - 역할: 프로그램의 코드, 데이터, 스택, 힙 등 논리적인 내용을 담고 있는 단위
        - 크기: 고정된 크기를 가지며, 페이지가 모여 하나의 프로세스를 구성
- 프레임
    - 실제 물리적 메모리 공간을 페이지와 동일한 크기로 나눈 단위를 의미
        - 위치: 실제 주기억장치의 물리적 주소 공간에 존재
        - 역할: 실제 데이터가 적재되는 물리적인 저장 공간 단위
        - 크기: 페이지와 동일한 크기를 가짐

**OS-097**

내부 단편화와, 외부 단편화에 대해 설명해 주세요.

내부 단편화

- 메모리를 고정된 크기의 블록으로 나눌 때 발생하는 메모리 낭비 현상
    - 할당된 메모리 공간 자체가 사용자가 실제로 필요로 하는 크기보다 커서, 블록 내부에 사용되지 않고 남아있는 공간이 생기는 것을 의미

외부 단편화

- 메모리를 가변적인 크기의 블록으로 나눌 때 발생하는 메모리 낭비 현상
    - 메모리 전체적으로는 사용 가능한 충분한 공간이 있지만, 이 공간들이 너무 작게 쪼개져 불연속적으로 존재하기 때문에 실제로 큰 프로세스에게 할당해 줄 수 없는 상태

**OS-098**

페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.

- **주소 변환** 과정을 통해 가져올 수 있음
    1. 가상 주소의 분해
    2. 페이지 테이블 참조
    3. 물리 주소 완성

⭐ **OS-099**

어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?

- 페이지 테이블 엔트리(PTE)를 확인하는 방법
    - 프로세스가 접근하는 모든 메모리 주소는 페이지 테이블을 거쳐 물리 주소로 변환됨 PTE에는 해당 주소 공간에 대한 권한 정보가 명시되어 있음
    - MMU - 주소 변환을 수행하는 동시에 해당 페이지의 W 비트를 확인. W 비트가 0읻네 쓰기를 시도하면, MMU는 페이지 보호 오류를 발생시키고 운영체제에 제어권을 넘김

**OS-100**

32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?

- 32비트 → 2^32 바이트 = 4GB
- 페이지 크기 = 1 KB = 2^10 바이트
- → 2^32 / 2^10 = 2^22

**OS-101**

32비트 운영체제는 램을 최대 4G 까지 사용할 수 있습니다. 이 이유를 페이징과 연관 지어서 설명해 주세요.

- 32비트 시스템에서 CPU의 레지스터, 명령어 및 주소 버스는 32비트 크기를 처리

**OS-102**

C/C++ 개발을 하게 되면 Segmentation Fault 라는 에러를 접할 수 있을텐데, 이 에러는 세그멘테이션/페이징과 어떤 관계가 있을까요?

- 널 포인터 역참조
- 유효하지 않은 포인터 접근
- 배열 인덱스 초과

---

**📌 TLB**

**OS-103**

TLB는 무엇인가요?

- 가상 주소를 실제 물리 주소로 변환하는 과정을 매우 빠르게 처리하기 위해 사용되는 고속 캐시 메모리
- MMU의 핵심 구성 요소

**OS-104**

TLB를 쓰면 왜 빨라지나요?

- 가상 메모리 환경에서는 CPU가 메모리 접근을 할 때마다 주소 변환이 필요함
    1. 페이지 테이블 참조: CPU가 가상 주소를 요청하면, 운영체제가 관리하는 페이지 테이블을 참조하여 해당 페이지가 RAM의 어느 프레임에 있는지 찾아야 함
    2. 이중 메모리 접근: 페이지 테이블 자체는 RAM에 저장되어 있음. 따라서 실제 데이터를 읽기 위해서는 RAM에 2번 접근해야 함
        - 1차 접근: 페이지 테이블을 읽기 위한 램 접근
        - 2차 접근: 실제 데이터를 읽기 위한 램 접근
        - 결과: 단 한 번의 데이터 접근에 두 번의 램 접근이 필요하게 되어 시스템 성능이 크게 저하
- TLB는 이 두번의 램 접근 오버헤드를 줄이기 위해 도입됨
    - TLB에 가상 페이지 번호와 실제 프레임 번호의 쌍을 저장
    - 요청된 페이지 번호가 TLB에 존재한다면, TLB에서 해당 프레임 번호를 바로 전달 (TLB Hit)
    - 요청된 페이지 번호가 TLB에 없다면, MMU는 RAM의 페이지 테이블에 접근하여 실제 프레임 번호를 얻음 (TLB Miss)

**OS-105**

MMU가 무엇인가요?

- CPU와 RAM 사이에 위치하는 하드웨어 장치로, 프로그램이 사용하는 가상 주소를 실제 RAM의 물리 주소로 변환하고, 메모리 접근을 보호하는 역할을 수행함

**OS-106**

TLB와 MMU는 어디에 위치해 있나요?

- 둘 다 CPU 내부에 위치하거나, CPU와 매우 가깝게 통합시킴
- TLB는 MMU의 주요 구성 요소로서 내장되어 있음

**OS-107**

코어가 여러개라면, TLB는 어떻게 동기화 할 수 있을까요?

- 멀티코어 시스템에서 각 코어는 자신만의 TLB를 가지고 있음. 따라서 하나의 코어가 페이지 테이블 정보를 변경했을 때, 다른 코어의 TLB에 있는 오래된 정보와 데이터 불일치 문제가 발생할 수 있음
- TLB간의 동기화는 하드웨어적 방법과 운영체제의 협력을 통해 이루어짐
1. 하드웨어 기반: TLB 스누핑
    - CPU의 각 코어는 공유 버스를 통해 다른 코어가 메모리 관리 영역에서 수행하는 작업을 엿듣고 감시함
2. 소프트웨어 기반: IPI와 OS 협력
    - IPI 사용
        - 한 코어가 페이지 테이블을 변경하면, 해당 코어는 IPI를 사용하여 다른 모든 코어에게 인터럽트를 발생시킴
        - 인터럽트를 받은 다른 코어들은 현재 하던 작업을 멈추고 TLB 무효화 함수를 실행하여, 자신의 TLB에서 변경된 페이지 엔트리를 삭제함

**OS-108**

TLB 관점에서, Context Switching 발생 시 어떤 변화가 발생하는지 설명해 주세요.

- 프로세스가 전환될 때마다 TLB에 있는 정보가 바뀌어야 하기 때문에 이를 해결하는 것이 중요.
- 그렇다고 TLB를 전체적으로 비우는 것은 성능을 저하시킴
- ASID: TLB 엔트리에 해당 변환 정보가 어떤 프로세스에 속하는지 식별하는 ID를 함께 저장
    - 프로세스가 전환되더라도 TLB를 완전히 비울 필요 없이, 새 프로세스에 해당하는 ASID의 정보만 사용함. 이는 TLB의 재사용성을 높여 성능을 유지하는 데 도움을 줌

**OS-109**

동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.

- 원자적 명령어를 활용
    1. 상호배제 보장 명령어 (원자적 연산)
        - CPU는 메모리의 특정 위치에 대한 읽기와 쓰기 작업을 분리될 수 없는 단일 연산으로 수행하도록 보장하는 특수 명령어를 제공 → 이를 통해 경쟁 조건을 원천적으로 차단
    2. 메모리 배리어
        - 성능 최적화를 위해 명령어 재배치를 수행. 하지만 이는 다중 스레드 환경에서 데이터 불일치를 일으킬 수 있음
        - 위 사항을 방지하기 위해 앞뒤의 메모리 접근 명령어 순서를 바꾸지 못하도록 강제함

**OS-110**

volatile 키워드는 어떤 의미가 있나요?

- 특정 변수가 컴파일러나 CPU에 의해 최적화되지 않도록 지시하는 예약어
- 해당 변수가 언제든지 외부 요인에 의해 예기치 않게 변경될 수 있음을 명시적으로 선언
- 사용에 주의해야 함

**OS-111**

싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요?

- 각 코어의 독립적인 캐시로 인해 발생하는 데이터 불일치 문제를 추가적으로 해결해야 함
1. 캐시 일관성 프로토콜
    - 스누핑 기반의 프로토콜 사용
2. 원자적 명령어
    - 읽기-수정-쓰기 과정을 분리 불가능한 단일 연산으로 처리하는 명령어 제공
3. 메모리 배리어
    - 앞뒤의 메모리 접근 명령어 순서를 바꾸지 못하도록 강제함
4. 뮤텍스
5. 세마포어
6. volatile 키워드

---

**📌 페이지 교체 알고리즘**

**OS-112**

페이지 교체 알고리즘에 대해 설명해 주세요.

- 가상 메모리 시스템에서 Page Fault가 발생하여 새로운 페이지를 메모리로 가져와야 할 때, 어떤 페이지 프레임을 비워서 새로운 페이지를 넣을지 결정하는 전략

**OS-113**

LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?

- 캐시 메모리 및 가상 메모리 관리의 핵심 원리인 시간적 지역성을 이용한 알고리즘

**OS-114**

LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?

- 

**OS-115**

LRU 알고리즘의 단점을 설명해 주세요. 이를 해결할 수 있는 대안에 대해서도 설명해 주세요.

- 어떤 페이지가 가장 오랫동안 사용되지 않았는지 정확하게 추적하려면 모든 페이지 접근 시간을 기록해야 하므로, 하드웨어 구현 비용이 매우 높고 오버헤드가 큼

---

**📌 파일 시스템 및 I/O**

**OS-116**

File Descriptor와, File System에 에 대해 설명해 주세요.

- File System
    - 컴퓨터의 보조 기억장치에 저장된 파일들을 효율적으로 관리하고 접근하기 위한 체계적인 방법 및 구조
- File Descriptor
    - 유닉스 및 리눅스 기반 OS에서 파일이나 다른 입출력 리소스에 접근할 때 사용하는 추상적인 식별자
    - 쉽게 말하면, 운영체제가 파일을 관리하기 위해 프로세스에 부여하는 고유한 정수

**OS-117**

I-Node가 무엇인가요?

- 유닉스 및 리눅스 계열 파일 시스템에서 파일이나 디렉토리에 대한 모든 메타데이터를 저장하는 핵심 데이터 구조
- 파일의 실제 데이터를 제외한 모든 정보를 관리하며, 운영체제가 파일을 식별하고 접근하는 데 사용되는 기본 단위

**OS-118**

프로그래밍 언어 상에서 제공하는 파일 관련 함수 (Python - open(), Java - BufferedReader/Writer 등)은, 파일을 어떤 방식으로 읽어들이나요?

- 시스템 콜을 이용해 디스크에 접근
- 효율성을 위해 버퍼링 메커니즘을 사용하며, 크게 커널 버퍼와 라이브러리 버퍼라는 두 단계를 거쳐 이루어짐
1. 커널 버퍼
    - I/O는 궁극적으로 운영체제 커널을 통해 이루어지기 때문에, 커널은 디스크 접근의 효율을 위해 시스템 전체에서 공유되는 메모리 영역을 버퍼로 사용
2. 라이브러리 버퍼
    - open() 함수를 통해 얻는 파일 객체나 Java의 `BufferedReader/Writer`와 같은 고수준(High-Level) 클래스들은 프로그램 내부 메모리에 별도의 버퍼를 두어 성능을 더욱 최적화

**OS-119**

동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.

- **동기(Synchronous)란?**
    
    > ‘동기’라고하면 다수의 개체들이 동일(일정)한 **무언가**를 가지는 것. 또한 **무언가**가 동일(일정)하게 되는 것.
    > 
    - 그 무언가는 `상태`가 될 수 있고 `행위`가 될 수 있고, `시간`, `속도`, `주기`, `출현` 등이 될 수 있다.
    - 여기서 말하는 ‘동기’는 두개의 프로세스가 데이터를 주고 받을 때, 주고 받는 `순서`(또는 `시간`)가 일정하다는 것을 뜻한다. (너 한번, 나 한번)
- **비동기(Asynchronous)란?**
    - ‘동기’가 아닌 것
- 동기식, 동기적이다.
    - 어떤 작업을 요청했을 때 그 작업이 종료될 때까지 기다린 후 다음 작업을 수행한다.
        - 데이터를 주고받는 ‘순서’가 중요할 때 사용된다.
        - 요청한 작업만 처리하면 되기 때문에 전체적인 수행 속도는 빠를 수 있다. (일만 하면 된다.)
        - 한 작업에 대한 시간이 길어질 경우, 전체 응답이 지연될 수 있다.
- 비동기식, 비동기적이다.
    - 어떤 작업을 요청했을 때 그 작업이 종료될 때까지 기다리지 않고(작업을 위임하고), 다음 작업을 수행한다. 요청했던 작업이 끝나면 결과를 받고, 그에 따른 추가 작업이 있다면 수행한다.
        - 요청 순서에 상관없이, 동시에 다수의 작업을 처리할 수 있다.
        - 작업이 끝날 때 따로 이벤트를 감지하고 결과를 받아 그에 따른 추가 작업을 해줘야하기 때문에, 비교적 느릴 수 있다.
        - I/O 작업이 잦고, 빠른 응담속도를 요구하는 프로그램에 적합하다.

---

- **블로킹**
    
    > 작업이 완료될 때까지 제어권을 반환하지 않는 방식.
    > 
    - 호출한 작업이 끝날 때까지 호출한 스레드는 멈춤
    - 작업이 완료될 때까지 다른 작업을 수행할 수 없으며, 리소스 낭비가 발생할 수 있음
    - 간단하고 직관적인 구현이 가능하지만 성능이 저하될 수 있음.
    - 파일 I/O, 소켓 I/O 등등 사용
- **논블로킹**
    
    > 작업이 완료되지 않아도 제어권을 즉시 반환하는 방식.
    > 
    - 호출한 작업이 완료되지 않아도 호출한 스레드는 멈추지 않고 다른 작업을 계속 진행
    - 작업을 요청한 후 바로 제어권을 반환하여 다른 작업을 수행할 수 있으며, 작업이 완료되면 별도의 콜백 함수나 이벤트를 통해 결과를 처리
    - 네트워크 서버, 이벤트 드리븐 시스템, 논블로킹 I/O, 멀티스레드 환경에서의 동시성 제어 등등 사용

---

**동기, 비동기, 블로킹, 논블로킹 차이점**

**[ 동기 vs 비동기 ]**

- 동기: 작업이 순차적으로 실행되며, 하나의 작업이 완료될 때까지 다음 작업 대기
- 비동기: 작업이 병렬적으로 실행되며, 작업이 완료되지 않아도 다른 작업을 동시에 실행

**[ 동기 vs 블로킹 ]**

- 동기 호출은 반드시 블로킹이 아닌 반면에 블로킹 호출은 모두 동기 호출임
- 동기: 어떤 함수를 동기 호출했다고 해서 블로킹되거나 스레드가 일시 중지되지 않는다.
- 블로킹, 동기: 어떤 함수를 동기 호출했다고 해서 블로킹되거나 스레드가 일시 중지된다.

**[ 동기 vs 논블로킹 ]**

- 동기적 요청 중 논블로킹 방식으로 데이터를 조회하거나 처리할 때 사용

**[ 비동기 vs 블로킹 ]**

- 이 조합은 일반적으로 사용되지 않으며, 비동기 작업의 본질과 맞지 않음

**[ 비동기 vs 논블로킹 ]**

- 비동기, 논블로킹: 네트워크 요청을 보내고 다른 작업하기
- 비동기, 논블로킹: 네트워크 요청을 보내고 특정 주기(10초, 1분, 5분) 확인하면서 다른 작업하기
- 논블로킹: 네트워크 요청을 보내고 다른 작업을 할 수 있는데 하지 않고 네트워크 응답이 왔는지만 확인하기

**[ 블로킹 vs 논블로킹 ]**

- 블로킹: 작업이 완료될때까지 제어권을 반환하지 않으며, 호출한 스레드는 멈춤
- 논블로킹: 작업이 완료되지 않아도 제어권을 즉시 반환하며, 호출한 스레드는 멈추지 않음

---

**동기, 비동기, 블로킹, 논블로킹 조합**

프로그램 아키텍처에서는 이 두 개념이 함께 조합되어 사용됨. 예를 들어 다음 4가지로 조합 가능

- 동기 + 블로킹 (Synchronous + Blocking)
- 동기 + 논블로킹 (Synchronous + Non-Blocking)
- 비동기 + 블로킹 (Asynchronous + Blocking)
- 비동기 +논블로킹 (Asynchronous + Non-Blocking)

![image.png](attachment:6a1bd754-a776-419e-9a14-830feee4587f:image.png)

**[ 동기 + 블로킹 ]**

동기 + 블로킹 조합은 다른 작업이 진행되는 동안 자신의 작업을 처리하지 않고 (Blocking), 다른 작업의 여부를 바로 받아 순차적으로 처리하는 (Sync) 방식이다. 다른 작업의 결과가 자신의 작업에 영향을 주는 경우에 활용할 수 있다. 예를 들어 파일을 읽는 함수가 호출되면, 파일을 모두 읽을 때까지 호출한 프로그램은 아무 작업도 하지 않고 기다림. 하지만 작업이 완료될 때까지 다른 작업을 할 수 없어 비효율적일 수 있음

**[ 동기 + 논블로킹 ]**

동기 + 논블로킹 조합은 다른 작업이 진행되는 동안에도 자신의 작업을 처리하고 (Non Blocking), 다른 작업의 결과를 바로 처리하지 않아 작업 순서가 지켜지지 않는 (Async) 방식이다. 다른 작업의 결과가 자신의 작업에 영향을 주지 않은 경우에 활용할 수 있다. 예를 들어 파일을 읽는 함수가 호출되면, 파일을 바로 읽기 시작하지만 호출자는 주기적으로 파일 읽기가 완료되었는지 확인해야 합니다. 하지만 주기적으로 상태를 확인해야 하므로 여전히 비효율적일 수 있음

**[ 비동기 + 블로킹 ]**

비동기 + 블로킹 조합은 다른 작업이 진행되는 동안 자신의 작업을 멈추고 기다리는 (Blocking), 다른 작업의 결과를 바로 처리하지 않아 순서대로 작업을 수행하지 않는 (Async) 방식이다. Async-blocking의 경우는 실무에서 잘 마주하기 쉽지 않아 다룰일이 거의 없다. 

**[ 비동기 + 논블로킹 ]**

비동기 + 논블로킹 조합은 다른 작업이 진행되는 동안에도 자신의 작업을 처리하고 (Non Blocking), 다른 작업의 결과를 바로 처리하지 않아 작업 순서가 지켜지지 않는 (Async) 방식이다. 다른 작업의 결과가 자신의 작업에 영향을 주지 않은 경우에 활용할 수 있다. 

**OS-120**

그렇다면, 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?

**[ 동기 + 블로킹 ]**

- **일반적인 파일 읽기:** 프로그램이 `read()` 함수를 호출하면, 파일의 모든 내용을 디스크에서 메모리로 읽어올 때까지 해당 스레드는 멈춰서 다음 코드를 실행하지 못합니다.
- **표준 입출력 (scanf, input):** 사용자 입력(키보드)을 기다릴 때, 입력이 완료될 때까지 프로그램은 멈춥니다.

**[ 동기 + 논블로킹 ]**

- **`select()`, `poll()` 시스템 콜 (I/O 멀티플렉싱):** 네트워크 소켓에서 데이터가 도착했는지 **주기적으로 확인**하는 방식. 데이터가 준비되지 않았다면 즉시 논블로킹으로 반환하고, 준비되었다면 동기적으로 데이터를 읽어 처리합니다.
- **게임 로딩 화면의 진행률 바:** 맵 데이터 로딩(오래 걸리는 작업) 중에도 화면의 로딩 바가 계속 움직이며 **진행 상태를 주기적으로 확인**하고 업데이트합니다.

**[ 비동기 + 논블로킹 ]**

- **Node.js의 I/O 작업:** 웹 서버가 클라이언트에게 대용량 파일 전송을 요청한 후, 파일 전송 완료를 기다리지 않고 다른 클라이언트 요청을 처리하며, 전송이 완료되면 지정된 콜백 함수가 실행됩니다.
- **AJAX/Fetch API 요청 (웹 브라우저):** 웹페이지에서 서버에 데이터를 요청할 때 (예: 좋아요 클릭), 페이지는 멈추지 않고 즉시 다른 콘텐츠를 렌더링하며, 서버 응답은 나중에 **Promise**나 **콜백**으로 처리됩니다.

**[ 비동기 + 블로킹 ]**

- 구현하기 매우 어려움
- 설계에서는 무의미하다고 간주됨

**OS-121**

I/O 멀티플렉싱에 대해 설명해 주세요.

- 하나의 단일 프로세스 또는 스레드가 여러 개의 입출력 장치에서 발생하는 I/O 이벤트를 동시에 감지하고 처리할 수 있도록 하는 기술
- 하나의 스레드가 여러 소켓을 감시함으로써, 동시 처리 능력을 높이고 시스템 자원 낭비를 줄임

**OS-122**

논블로킹 I/O를 수행한다고 하면, 그 결과를 어떻게 수신할 수 있나요?

- 작업을 요청한 후 바로 제어권을 반환하여 다른 작업을 수행할 수 있으며, 작업이 완료되면 별도의 콜백 함수나 이벤트를 통해 결과를 처리
