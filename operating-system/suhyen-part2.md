# Operating System (운영체제) - Part2

## 📌 IPC

### OS-064
<details>
<summary>IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요.</summary>
<hr>

IPC(Inter-Process Communication)는 프로세스가 데이터를 주고받는 방법을 의미합니다. 프로세스는 서로 독립된 메모리 공간을 가지고 있기 때문에, 데이터를 공유하거나 통신하려면 IPC 기법을 사용해야 합니다.

주요 기능으로는,

1. 파이프(Pipes): 단방향 통신 메커니즘으로, 부모-자식 프로세스 간 통신에 주로 사용.
> 익명 파이프: 같은 부모를 가지는 프로세스 간에만 사용.
> 
> 이름 있는 파이프(named pipe): 서로 다른 부모를 가진 프로세스 간에도 사용.

2. 메시지 큐(Message Queue): 메시지를 큐에 저장하고 프로세스가 비동기적으로 통신할 수 있도록 하는 메커니즘.
> 메시지를 특정 키로 식별하여, 송신 프로세스는 메시지를 큐에 추가하고, 수신 프로세스는 큐에서 메시지를 꺼낸다.

3. 공유 메모리(Shared Memory): 여러 프로세스가 공통 메모리 공간을 공유하여 데이터를 주고받는 방식.
> 가장 빠른 IPC 방식 중 하나. 대량 데이터 통신에 적합하지만 동기화가 필요(뮤텍스, 세마포어 사용).

4. 소켓(Sockets): 네트워크를 통한 통신을 지원하는 메커니즘으로 같은 시스템 내의 프로세스나 네트워크 상의 다른 시스템 간의 통신을 지원.
> 클라이언트-서버 모델에서 주로 사용되며, 양방향 통신이다.

5. 세마포어(Semaphores): 프로세스 간 동기화를 위한 기법.
   
6. 파일(File): 프로세스가 파일 시스템을 통해 데이터를 공유하는 방식으로 각 프로세스는 같은 파일을 열어 데이터를 읽고 쓸 수 있다.

</details>

### OS-065
<details>
<summary>Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.</summary>
<hr>

Shared Memory(공유 메모리)는 두 개 이상의 프로세스가 같은 메모리 공간을 공유하여 데이터를 주고받는 방식입니다.

하나의 프로세스가 생성한 공유 메모리 영역에 다른 프로세스가 접근하여 데이터를 읽거나 쓸 수 있기 때문에 데이터 복사를 하지 않아서 다른 IPC방식보다 빠르다는 장점을 가집니다.

다만, 프로세스가 동시에 공유 메모리 영역에 접근하면 경쟁 상태가 발생할 수 있으므로    
동기화 기법이 필요하고 일반적인 프로세스의 메모리와 다르게 운영체제의 커널에서 관리되기 때문에 명시적으로 할당하고 해제해야 합니다.
> 일반적인 상황: 프로세스 간의 메모리 접근을 분리하여, 한 프로세스가 다른 프로세스의 메모리에 접근하지 못하게 보호
>
> 공유메모리 상황: 특정 프로세스들 간에 메모리를 공유하므로 운영체제가 자동으로 관리하는 게 아님

![image](./images/suhyen-OS-065(1).png)

> 추가) 과거의 프로그래머들은 프로세스가 자주 사용하는 명령어가 특정한 Code에 집중되어 있다는 사실을 깨닫게 되었고, 그 해당 Code 부분을 따로 분리하여 사용하면 메모리를 절약할 수 있겠다는 생각을 하게 되었다. 이렇게 해서 탄생한 것이 라이브러리(Library)이다.

</details>

### OS-066
<details>
<summary>메시지 큐는 단방향이라고 할 수 있나요?</summary>
<hr>

기본적으로 하나의 큐에서 한 프로세스가 메시지를 전송하고 다른 프로세스가 수신하는 단방향 통신 메커니즘으로 사용할 수 있지만    
여러 큐를 사용하여 양방향 통신도 가능한 구조로 만들 수 있습니다. 
> 두 개의 메시지 큐를 사용하여 프로세스 A와 B가 각각 서로에게 메시지를 보낼 수 있도록 두 개의 메시지 큐를 사용하면, 양방향 통신이 가능하다.

</details>

## 📌 스레드 안전성

### OS-067
<details>
<summary>Thread Safe하다는 것은 어떤 의미인가요?</summary>
<hr>

Thread Safe는 여러 스레드가 동시에 실행되더라도 데이터의 일관성과 프로그램의 정상적인 동작을 보장하는 것을 의미합니다. 특정 코드, 함수, 또는 객체가 동시 접근 시에도 예기치 않은 동작을 하지 않음을 뜻합니다.

</details>

### OS-068
<details>
<summary>Thread Safe를 보장하기 위해 어떤 방법을 사용할 수 있나요?</summary>
<hr>

Thread Safe를 보장하는 방법은 공유 자원에 대한 동기화와 경쟁 상태를 방지하는 데 초점을 둡니다.

- **Mutual Exclusion (상호 배제)**:  
  - 락(Mutex, Semaphore)을 사용하여 공유 자원에 동시 접근을 방지함.  
  - Monitor를 이용해 객체 수준에서 동기화를 보장함.
  
- **Atomic Operations (원자적 연산)**:  
  - 원자적 연산을 사용하여 데이터의 일관성을 유지함. 예: `std::atomic` (C++), `AtomicInteger` (Java).  
  
- **Thread Local Storage (TLS)**:  
  - 스레드별로 독립적인 데이터 공간을 제공하여 공유 자원 문제를 방지함.  

- **Immutable Objects (불변 객체)**:  
  - 데이터를 불변으로 설계하여 동시 접근에서의 변경을 차단함.  

- **Message Passing**:  
  - 공유 메모리를 사용하는 대신 메시지 큐를 통해 데이터를 전달하는 방법

</details>

### OS-069
<details>
<summary>Peterson's Algorithm이 무엇이며, 한계점에 대해 설명해 주세요.</summary>
<hr>

**Peterson's Algorithm**은 두 개의 스레드 간 상호 배제를 보장하는 알고리즘입니다. 

- **작동 원리**:  
  - 각 스레드가 임계 구역에 진입하려고 할 때, 자신의 플래그를 `true`로 설정하고 상대 스레드의 플래그와 우선권 변수를 확인함.  
  - 상대 스레드가 우선권을 가지면 대기하고, 그렇지 않으면 임계 구역에 진입함.  

- **장점**:  
  - 락 없이 소프트웨어적으로 구현 가능하며, 상호 배제와 진행 보장을 제공함.  

- **한계점**:  
  - **두 스레드 제한**: 두 개 이상의 스레드에서는 작동하지 않음.  
  - **Busy Waiting**: 대기 상태에서도 CPU를 소모하며 효율적이지 않음.  
  - **현대 하드웨어 비효율성**: 캐시 및 메모리 모델과 호환성이 떨어짐.  

</details>

### OS-070
<details>
<summary>Race Condition이 무엇인가요?</summary>
<hr>

**Race Condition**은 여러 스레드가 공유 자원에 동시에 접근할 때 발생하며, 실행 순서에 따라 결과가 달라질 수 있는 상황을 의미합니다. 이는 데이터 불일치나 프로그램 오류를 유발할 수 있습니다.

- **예**:  
  ```c
  int counter = 0;

  void increment() {
      counter++;
  }
  ```
  위 함수가 여러 스레드에서 호출되면, `counter` 값이 예상과 다르게 증가할 수 있음.  

- **해결 방법**:  
  - 락, 세마포어, 원자적 연산 등을 사용해 스레드 간 접근을 동기화함.

</details>

### OS-071
<details>
<summary>Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?</summary>
<hr>

대체하는 방법들이 다수 존재합니다.

- **Atomic Operations (원자적 연산)**:  
  - `std::atomic`(C++), `AtomicInteger`(Java)와 같은 원자적 연산을 사용하여 락 없이도 Thread Safe를 구현할 수 있음.  

- **Lock-Free Data Structures**:  
  - 락 대신 CAS(Compare-And-Swap) 등의 기법으로 동기화를 수행함.  

- **Thread Local Storage (TLS)**:  
  - 스레드별 독립적인 데이터 공간을 제공하여 공유 자원을 피함.  

- **Immutable Objects (불변 객체)**:  
  - 데이터를 불변으로 설계하여 동시 접근에서의 변경을 차단함.  

- **Message Passing**:  
  - 공유 상태를 없애고 메시지 큐로 데이터를 전달하는 방법

</details>

## 📌 Thread Pool, Monitor, Fork-Join

### OS-072
<details>
<summary>Thread Pool, Monitor, Fork-Join에 대해 설명해 주세요.</summary>
<hr>

### **Thread Pool**  
Thread Pool은 미리 생성된 스레드의 집합으로, 작업 요청이 들어오면 풀에 있는 스레드 중 하나가 작업을 처리하는 방식이다.  
- **특징**: 작업이 완료되면 스레드는 풀로 반환되며 재사용된다.  
- **장점**:  
  - 스레드 생성 및 제거에 따른 오버헤드를 줄인다.  
  - 적절한 스레드 수를 유지하여 시스템 성능을 최적화한다.  
- **활용 예**:  
  - 웹 서버에서 클라이언트 요청 처리, 데이터베이스 연결 관리 등.  


### **Monitor**  
Monitor는 상호 배제와 스레드 간 동기화를 제공하며, 특정 조건이 만족될 때까지 스레드를 기다리게 하거나 실행을 제어하는 메커니즘이다.  
- **특징**:  
  - 자바에서는 `synchronized` 키워드로 구현된다.  
  - 모든 객체는 monitor를 가지고 있으며, 하나의 객체에 하나의 락이 할당된다.  
- **구현 방식**:  
  - `wait`, `notify`, `notifyAll` 메서드로 스레드 상태를 관리한다.  
  - 락과 조건 변수를 통해 공유 자원에 대한 접근을 제어한다.  
- **활용 예**:  
  - 멀티스레드 환경에서 공유 리소스 보호, 조건 만족 시 스레드 실행 관리.  


### **Fork-Join**  
Fork-Join은 큰 작업을 여러 개의 작은 작업으로 나누고(Fork), 각각을 병렬로 처리한 뒤 결과를 합치는(Join) 프레임워크다.  
- **특징**:  
  - Divide and Conquer(분할 정복) 알고리즘을 기반으로 한다.  
  - 작업을 효율적으로 병렬화하여 다중 코어 시스템에서 성능을 극대화한다.  
- **활용 예**:  
  - 병렬 정렬, 병렬 검색 등 대규모 작업의 병렬 처리.  
  - 자바의 `ForkJoinPool`이 대표적인 구현체다.  

</details>

### OS-073
<details>
<summary>Thread Pool을 사용한다고 가정하면, 어떤 기준으로 스레드의 수를 결정할 것인가요?</summary>
<hr>


스레드 수는 작업의 성격(연산 중심인지, I/O 중심인지)에 따라 다르게 결정된다.  

1. **CPU-바운드 작업**  
   - 대부분 CPU 연산에 의존하는 작업이다.  
   - 권장 스레드 수: 스레드 수 = CPU 코어 수 + 1 
   - **이유**:  
     - 스레드가 많아질수록 컨텍스트 스위칭 오버헤드가 증가하므로 CPU 코어 수에 맞게 스레드 수를 제한해야 한다.  
   - **예**: 데이터 처리, 영상 처리 등.  

2. **I/O-바운드 작업**  
   - I/O 작업 중 대기 시간이 많다.  
   - 권장 스레드 수: 스레드 수 = CPU 코어 수 * (1 + I/O대기시간 / CPU처리시간)
   - **이유**:  
     - 대기 시간이 많으므로 추가적인 스레드를 통해 병렬 처리를 최적화할 수 있다.  
   - **예**: 네트워크 요청 처리, 파일 읽기/쓰기.  

3. **혼합 작업**  
   - CPU와 I/O 작업이 혼합된 경우이다.  
   - **권장 방법**: 작업의 CPU 및 I/O 비율을 분석한 뒤, 적절한 스레드 수를 설정해야 한다.  
   - **방법**: 실험적 테스트 및 성능 모니터링을 통해 최적 스레드 수를 결정한다.  


</details>

### OS-074
<details>
<summary>어떤 데이터를 정렬하려고 합니다. 어떤 방식의 전략을 사용하는 것이 가장 안전하면서도 좋은 성능을 낼 수 있을까요?</summary>
<hr>

데이터 정렬 전략은 데이터 크기, 안정성 요구, 병렬 처리 가능성에 따라 달라집니다.

1. **안정성을 요구하고, 데이터 크기가 작거나 중간인 경우**  
   - **Merge Sort**  
     - 안정적이며, 메모리 사용량이 예측 가능하다.  
     - Fork-Join과 결합하면 병렬 정렬로 성능을 높일 수 있다.  
   - **Timsort**  
     - Merge Sort와 Insertion Sort를 결합한 알고리즘으로, 실세계 데이터에서 매우 효율적이다.  

2. **안정성을 요구하지 않는 경우**  
   - **Quick Sort**  
     - 평균적으로 \( O(n \log n) \)의 성능을 보이며, 메모리 사용량이 적다.  
     - 데이터 분포가 균등하지 않은 경우 Pivot 선택을 최적화하여 최악의 시간 복잡도 \( O(n^2) \)을 피한다.  

3. **대규모 데이터 또는 외부 메모리가 필요한 경우**  
   - **External Sort (외부 정렬)**  
     - 대용량 데이터를 처리할 때 디스크 I/O를 최소화하며, 병렬로 데이터를 정렬하고 병합한다.  
   - **Fork-Join 기반 Merge Sort**  
     - 데이터를 작은 청크로 나누어 병렬로 정렬한 뒤 병합한다.  

4. **병렬 처리를 활용하려는 경우**  
   - **Fork-Join과 병렬 정렬**  
     - 병렬 Merge Sort나 Quick Sort를 사용하여 다중 코어 환경에서 성능을 극대화한다.  
   - **Java Parallel Streams**  
     - Java의 병렬 스트림 기능을 활용해 데이터를 빠르게 정렬할 수 있다.  

- **데이터 크기가 크고 안정성이 중요한 경우**: Fork-Join 기반 Merge Sort.  
- **메모리 제약이 있는 대규모 데이터**: External Sort.  
- **소규모 데이터 또는 빠른 정렬이 필요한 경우**: Quick Sort 또는 Timsort.  

</details>

## 📌 캐시

### OS-075
<details>
<summary>캐시 메모리 및 메모리 계층성에 대해 설명해 주세요.</summary>
<hr>

![image](./images/suhyen-OS-075(1).png)


캐시 메모리는 CPU와 메인 메모리(RAM) 사이에 위치하며, 데이터 접근 속도를 향상시키기 위해 사용하는 고속 메모리입니다.  
메모리 계층성은 속도와 용량의 균형을 위해 설계된 계층적 구조로, CPU 레지스터, 캐시 메모리(L1, L2, L3), 메인 메모리, 디스크(SSD/HDD) 순으로 이루어진다.  


- **속도**: 상위 계층(레지스터, 캐시)일수록 빠르다.  
- **용량**: 하위 계층(메인 메모리, 디스크)일수록 크다.  
- **비용**: 상위 계층일수록 단가가 높다.

</details>

### OS-076
<details>
<summary>캐시 메모리는 어디에 위치해 있나요?</summary>
<hr>

캐시 메모리는 CPU 내부 또는 CPU와 메인 메모리(주기억장치) 사이에 위치합니다.  
L1, L2 캐시는 CPU 내부에, L3 캐시는 CPU 내부 또는 외부에 위치합니다.  
![image](./images/suhyen-OS-076(1).png)

</details>

### OS-077
<details>
<summary>L1, L2 캐시에 대해 설명해 주세요.</summary>
<hr>

- **L1 캐시**:  
  - CPU 코어에 가장 가까운 캐시로, 속도가 가장 빠르고 용량이 가장 작다.  
  - 일반적으로 32KB~128KB 정도의 크기를 가지며, 명령어와 데이터를 별도로 저장하는 구조로 나뉘는 경우가 많다.  

- **L2 캐시**:  
  - L1 캐시보다 크며, 비교적 느리지만 여전히 매우 빠르다.  
  - 용량은 일반적으로 수백 KB에서 수 MB 수준이다.  
  - L1 캐시와 CPU 코어 사이의 데이터를 관리하며, 여러 코어가 공유하지 않고 독립적으로 사용한다.

> 여기서 L은 Level을 의미.

> 캐시 메모리 용량에 따른 성능 차이는 벤치마크 툴로 테스트할 때 수치로 증명될 뿐이지, 실제로 일반적으로 사용하면서 체감할 수 있을 정도는 아니라고 한다.

</details>

### OS-078
<details>
<summary>캐시에 올라오는 데이터는 어떻게 관리되나요?</summary>
<hr>

캐시에 올라오는 데이터는 **블록 단위**로 관리됩니다.

- **캐시 라인**: 캐시 메모리는 고정된 크기의 데이터 블록(캐시 라인) 단위로 데이터를 저장한다.  
- **캐시 정책**:  
  - **Write-through**: 데이터 변경 시 메모리와 캐시에 동시에 기록한다.  
  - **Write-back**: 데이터 변경 시 캐시에만 기록하고, 메모리에 반영은 나중에 한다.  
- **교체 알고리즘**: 새로운 데이터를 저장할 공간이 부족할 때, 기존 데이터를 교체하기 위한 알고리즘을 사용한다. 대표적으로 LRU(Least Recently Used), FIFO(First In First Out) 등이 있다.

</details>

### OS-079
<details>
<summary>캐시간의 동기화는 어떻게 이루어지나요?</summary>
<hr>

CPU 캐시와 메인 메모리 간 데이터 일관성을 유지하기 위해 이루어진다. 

### 캐시 기법들
- **Write-through**: 데이터 변경 시 메인 메모리에도 즉시 반영해 일관성을 유지한다.  
- **Write-back**: 변경된 데이터를 캐시에만 저장하고, 메모리 동기화는 나중에 필요할 때 수행한다.  
- **캐시 코히어런시**: 멀티코어 환경에서 동일 데이터의 복사본을 가진 캐시들이 일관성을 유지하기 위해 MESI(Modified, Exclusive, Shared, Invalid) 프로토콜과 같은 기법을 사용한다.

</details>

### OS-080
<details>
<summary>캐시 메모리의 Mapping 방식에 대해 설명해 주세요.</summary>
<hr>

캐시 메모리의 Mapping 방식은 메모리 데이터를 캐시에 저장하기 위한 규칙을 정의한다.
- **Direct Mapping**:  
  - 메모리 블록이 특정 캐시 라인에 고정적으로 매핑된다.  
  - 구현이 간단하고 빠르지만 충돌이 발생할 가능성이 크다.  

- **Fully Associative Mapping**:  
  - 메모리 블록이 캐시의 어느 라인에나 저장될 수 있다.  
  - 유연성이 높지만 매핑 과정에서 비교 연산 비용이 증가한다.  

- **Set Associative Mapping**:  
  - 캐시를 여러 개의 집합(Set)으로 나누고, 각 집합 내에서 Fully Associative 방식을 적용한다.
  - Direct Mapping과 Fully Associative Mapping의 절충안으로, 성능과 구현 복잡도의 균형을 맞춘 방식

</details>

### OS-081
<details>
<summary>캐시의 지역성에 대해 설명해 주세요.</summary>
<hr>

캐시의 지역성은 프로그램이 데이터에 접근하는 패턴을 기반으로, 특정 데이터가 캐시에 존재할 가능성을 높이는 특성을 의미한다.  
- **시간 지역성**: 최근에 접근한 데이터는 가까운 미래에 다시 접근될 가능성이 높다.  
- **공간 지역성**: 접근한 데이터와 가까운 주소의 데이터가 곧 접근될 가능성이 높다.

</details>

### OS-082
<details>
<summary>캐시의 지역성을 기반으로, 이차원 배열을 가로/세로로 탐색했을 때의 성능 차이에 대해 설명해 주세요.</summary>
<hr>

이차원 배열에서 가로 탐색은 캐시의 공간 지역성을 활용하므로 성능이 더 좋다.  
- **가로 탐색**: 메모리에 연속적으로 저장된 데이터에 접근하므로, 캐시 라인 히트율이 높다.  
- **세로 탐색**: 메모리에서 비연속적인 주소에 접근하므로, 캐시 미스가 발생할 가능성이 크다.

</details>

### OS-083
<details>
<summary>캐시의 공간 지역성은 어떻게 구현될 수 있을까요? (힌트: 캐시는 어떤 단위로 저장되고 관리될까요?)</summary>
<hr>

캐시의 공간 지역성은 데이터를 **캐시 라인 단위**로 관리함으로써 구현된다.  
- 캐시는 메모리에서 데이터를 가져올 때, 단일 값이 아닌 인접한 데이터 블록(캐시 라인)을 한 번에 로드한다.  
- 예를 들어, 한 번의 메모리 접근으로 64바이트 크기의 데이터 블록이 캐시에 저장된다.  
- 이를 통해 연속적인 데이터 접근에서 캐시 히트율을 높일 수 있다.

</details>

## 📌 메모리 할당

### OS-084
<details>
<summary>연속할당 방식 세 가지를 설명해주세요. (first-fit, best-fit, worst-fit)</summary>
<hr>

### **1. First-fit**  
First-fit은 메모리 할당 요청 시, 가용 공간 중에서 첫 번째로 적합한 크기의 블록에 할당하는 방식이다.  
- **장점**: 검색을 중단하는 조건이 단순하여 속도가 빠르며, 메모리 관리의 오버헤드가 적다.  
- **단점**: 연속된 메모리의 사용으로 인해, 시간이 지남에 따라 메모리 단편화가 심화될 가능성이 있다.  


### **2. Best-fit**  
Best-fit은 요청된 크기보다 같거나 큰 가용 블록 중에서, 가장 작은 크기의 블록에 메모리를 할당하는 방식이다.  
- **장점**: 할당 후 남은 메모리의 크기를 최소화하여 외부 단편화를 줄이는 데 유리하다.  
- **단점**: 작은 크기의 가용 공간이 다수 생성되면서 오히려 사용 가능한 메모리 공간이 비효율적으로 분할될 가능성이 있다.  


### **3. Worst-fit**  
Worst-fit은 요청된 크기보다 같거나 큰 가용 블록 중에서, 가장 큰 크기의 블록에 메모리를 할당하는 방식이다.  
- **장점**: 큰 블록을 분할하여 가용 공간을 남기기 때문에 추가적인 요청에 더 큰 메모리 블록을 제공할 가능성이 있다.  
- **단점**: 큰 블록이 불필요하게 나누어져, 실제 메모리 사용 효율성이 떨어질 가능성이 있다.  

</details>

### OS-085
<details>
<summary>worst-fit 은 언제 사용할 수 있을까요?</summary>
<hr>

1. **크기가 큰 메모리 요청이 빈번한 경우**  
   - Worst-fit은 가장 큰 블록을 선택하여 할당하기 때문에, 큰 크기의 요청이 들어왔을 때 비교적 더 적합한 공간을 제공할 가능성이 높다.  
2. **메모리 요청의 크기가 다양하게 혼재된 경우**  
   - 다양한 크기의 메모리 요청을 처리하면서 큰 블록을 유지하려는 전략이 필요할 때 활용할 수 있다.  
3. **외부 단편화에 대한 우려가 상대적으로 낮은 환경**  
   - Worst-fit은 외부 단편화가 심화될 가능성이 높지만, 단편화의 영향이 크지 않거나 특정 목적을 위한 메모리 사용 환경에서는 선택지가 될 수 있다.  

</details>

### OS-086
<details>
<summary>성능이 가장 좋은 알고리즘은 무엇일까요?</summary>
<hr>

- **First-fit**: 성능 면에서 가장 효율적이다. 검색 과정을 단순화하여 시간 복잡도를 줄이며, 일반적인 메모리 요청 패턴에서 실용적이다.  
- **Best-fit**: 단편화를 줄이는 데 유리하지만, 작은 공간의 단편화가 누적될 경우 성능에 영향을 미친다.  
- **Worst-fit**: 특정 조건에서 유용할 수 있으나, 일반적인 환경에서는 메모리 활용의 효율성이 낮아지는 경향이 있다.  

따라서, **First-fit**이 일반적으로 성능이 가장 뛰어난 것으로 평가되며, Best-fit은 단편화를 관리해야 하는 상황에서 우선시된다. Worst-fit은 극히 제한된 특수 상황에서만 선택될 가능성이 높다.

</details>

## 📌 Thrashing

### OS-087
<details>
<summary>Thrashing 이란 무엇인가요?</summary>
<hr>

![image](./images/suhyen-OS-087(1).png)

Thrashing은 **가상 메모리 시스템**에서 페이지 부재(Page Fault)가 과도하게 발생하여 CPU가 실제 작업을 수행하기보다는 페이지 교체에만 대부분의 시간을 소비하는 현상을 의미한다.  
- **원인**:  
  - 프로세스에 필요한 메모리보다 가용 메모리가 부족할 때 발생한다.  
  - 과도한 멀티태스킹, 과도하게 작은 페이지 프레임 수 등이 주요 원인이다.  
- **결과**:  
  - 시스템 성능이 급격히 저하되고 응답 시간이 길어진다.  

</details>

### OS-088
<details>
<summary>Thrashing 발생 시, 어떻게 완화할 수 있을까요?</summary>
<hr>

Thrashing을 효과적으로 완화하기 위해서는 메모리와 프로세스 관리 정책을 최적화해야 한다. 

### **1. 워킹 셋 관리**  
Thrashing 완화에서 가장 중요한 것은 **워킹 셋(Working Set)** 을 관리하는 것이다. 워킹 셋은 프로세스가 일정 시간 동안 자주 참조하는 페이지들의 집합으로, 이를 메모리에 유지하면 Thrashing을 줄일 수 있다.  
- **방법**:  
  - 워킹 셋 모델을 사용하여 필요한 페이지들이 메모리에 유지되도록 한다.  
  - 페이지 부재율을 기반으로 프레임 수를 조정하여 워킹 셋 크기를 보장한다.  
- **효과**:  
  - 프로세스의 메모리 요구량을 충족시키고, 페이지 교체 빈도를 줄여 Thrashing 발생을 예방한다.



### **2. 프로세스 수 줄이기**  
메모리 부족 시, 실행 중인 프로세스 수를 줄여 각 프로세스가 사용할 수 있는 프레임 수를 확보해야 한다.  
- **방법**:  
  - 우선순위가 낮은 프로세스를 Swap Out하거나 일시 중단시킨다.  
  - 중요도가 높은 프로세스를 유지하면서 메모리 사용을 최적화한다.  
- **효과**:  
  - 메모리 리소스를 확보하여 각 프로세스가 필요한 최소한의 프레임을 보장할 수 있다.


### **3. 페이지 교체 알고리즘 최적화**  
효율적인 페이지 교체 알고리즘을 사용하여 페이지 부재율을 최소화할 수 있다.  
- **방법**:  
  - **LRU (Least Recently Used)**: 가장 오랫동안 사용되지 않은 페이지를 교체하여 Thrashing을 줄인다.  
  - **Clock 알고리즘**: LRU를 효율적으로 구현한 방식으로, 페이지 교체 비용을 낮춘다.  
- **효과**:  
  - 적절한 페이지 교체를 통해 시스템 성능과 메모리 효율을 향상시킨다.


### **4. 가상 메모리 크기 조정**  
메모리 부족으로 인한 Thrashing을 줄이기 위해 가상 메모리 크기를 조정하거나 물리 메모리를 증설해야 한다.  
- **방법**:  
  - 가상 메모리 크기를 늘려 페이지 교체가 자주 발생하지 않도록 설정한다.  
  - 물리 메모리(RAM)를 증설하여 가상 메모리 의존도를 줄인다.  
- **효과**:  
  - 메모리 부족 문제를 해결하여 Thrashing 가능성을 감소시킨다.


### **5. I/O 성능 개선**  
페이지 교체와 관련된 디스크 I/O 병목을 줄여 Thrashing의 부작용을 완화할 수 있다.  
- **방법**:  
  - SSD와 같은 빠른 스토리지를 사용하여 페이지 교체 속도를 높인다.  
  - 캐시 디스크나 파일 시스템 최적화를 통해 페이지 부재 처리 속도를 개선한다.  
- **효과**:  
  - 페이지 교체 속도를 개선해 Thrashing으로 인한 응답 지연을 줄인다.

</details>

## 📌 가상 메모리

### OS-089
<details>
<summary>가상 메모리란 무엇인가요?</summary>
<hr>

가상 메모리는 프로세스가 사용하는 주소 공간을 실제 물리 메모리 크기에 구애받지 않고 확장할 수 있도록 설계된 메모리 관리 기법입니다. 운영체제가 디스크를 활용해 물리 메모리를 보완하고, 프로그램이 필요한 데이터를 필요한 시점에만 물리 메모리에 적재함으로써 메모리를 효율적으로 사용할 수 있게 합니다. 이를 통해 메모리 부족 문제를 완화하고, 다중 프로세스 실행 시 시스템의 안정성과 성능을 유지합니다.

</details>

### OS-090
<details>
<summary>가상 메모리가 가능한 이유가 무엇일까요?</summary>
<hr>

가상 메모리가 가능한 이유는 하드웨어와 소프트웨어의 협업덕분입니다. 

1. MMU(Memory Management Unit): CPU와 메모리 사이에서 논리 주소를 물리 주소로 변환하는 하드웨어 장치입니다. 이를 통해 가상 주소와 실제 메모리 주소 간의 매핑이 가능합니다.
2. 페이지 기반 메모리 관리: 운영체제가 메모리를 고정된 크기의 페이지 단위로 관리하며, 각 페이지는 독립적으로 물리 메모리에 매핑될 수 있습니다.
3. 스왑 공간: 디스크의 일부를 스왑 공간으로 활용해 물리 메모리 크기를 초과하는 주소 공간을 제공합니다.
4. 운영체제의 페이지 테이블 관리: 운영체제가 각 프로세스의 가상 주소와 물리 주소 매핑 정보를 효율적으로 관리하기 때문에 프로세스 간의 충돌을 방지하고 가상 메모리를 지원할 수 있습니다.

</details>

### OS-091
<details>
<summary>Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.</summary>
<hr>

Page Fault는 프로세스가 참조하려는 페이지가 물리 메모리에 존재하지 않을 때 발생하는 이벤트입니다.

1. **Page Fault 인터럽트 발생**
   - CPU는 해당 참조를 처리할 수 없으므로 운영체제에 인터럽트를 요청합니다.
2. **페이지 존재 여부 확인**
   - 운영체제는 페이지 테이블을 참조해 요청한 페이지가 디스크에 존재하는지, 아니면 잘못된 참조(Invalid Reference)인지 판단합니다.
3. **페이지 로드 수행**
   - 디스크에 존재하는 페이지라면, 물리 메모리에 빈 프레임을 확보하거나 페이지 교체 알고리즘(e.g., LRU, FIFO 등)을 통해 기존 페이지를 교체합니다.
   - 페이지를 물리 메모리에 적재합니다.
4. **페이지 테이블 업데이트**
   - 새로운 페이지의 물리 주소를 페이지 테이블에 기록하고, 해당 페이지를 유효한 상태로 설정합니다.
5. **프로세스 재개**
   - 중단되었던 프로세스는 Page Fault 처리 이후 명령어를 다시 실행합니다.

</details>

### OS-092
<details>
<summary>페이지 크기에 대한 Trade-Off를 설명해 주세요.</summary>
<hr>

- **페이지 크기가 작을 경우**
  - **장점**
    - 메모리 낭비(내부 단편화)가 최소화됩니다. 실제로 필요한 데이터만 물리 메모리에 적재하므로 자원을 효율적으로 사용합니다.
  - **단점**
    - 페이지 테이블 크기가 커집니다. 많은 페이지를 관리해야 하므로 테이블 오버헤드와 주소 변환 시간이 증가합니다.
    - 디스크 I/O가 빈번해져 성능이 저하될 수 있습니다.

- **페이지 크기가 클 경우**
  - **장점**
    - 페이지 테이블 크기가 작아지고, 디스크에서 메모리로의 데이터 전송이 더 효율적입니다. 큰 단위로 읽어들임으로써 I/O 성능을 향상시킬 수 있습니다.
  - **단점**
    - 메모리 낭비(내부 단편화)가 증가합니다. 실제로 필요한 데이터보다 더 큰 페이지가 로드되면서 비효율이 발생할 가능성이 높습니다.

</details>

### OS-093
<details>
<summary>페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?</summary>
<hr>

페이지 크기가 커질수록 일반적으로 페이지 폴트는 감소합니다. 이는 큰 페이지 크기 덕분에 한 번의 페이지 로드로 더 많은 데이터가 메모리에 적재되어 참조 지역성(Locality of Reference)을 더 효과적으로 활용할 수 있기 때문입니다.

다만, 불필요한 데이터까지 메모리에 로드되는 경우 내부 단편화가 발생하며, 전체 메모리 사용 효율은 저하될 수 있습니다. 혹은, 과도하게 큰 페이지 크기를 설정하면, 오히려 물리 메모리의 가용량이 제한되거나 빈번한 페이지 교체로 이어질 수 있습니다.

</details>

### OS-094
<details>
<summary>세그멘테이션 방식을 사용하고 있다면, 가상 메모리를 사용할 수 없을까요?</summary>
<hr>

상호배타적인 개념이 아니기 때문에 결합해서 가능합니다.

1. 세그먼트를 다시 페이지 단위로 나누어 관리하면, 세그멘테이션의 논리적 메모리 구분 장점과 페이징의 물리 메모리 관리 효율성을 동시에 활용할 수 있습니다.
2. 세그멘테이션 테이블과 페이지 테이블을 조합해 가상 주소를 물리 메모리로 매핑할 수 있으며, 이 경우 각 세그먼트 내 페이지가 독립적으로 스왑 인/아웃됩니다.

</details>

## 📌 세그멘테이션 & 페이징

### OS-095
<details>
<summary>세그멘테이션과 페이징의 차이점은 무엇인가요?</summary>
<hr>

1. **메모리 단위**
   - 세그멘테이션: 메모리를 논리적 단위(코드, 데이터, 스택 등)로 구분하며, 세그먼트 크기는 가변적입니다.
   - 페이징: 메모리를 고정된 크기의 페이지로 나눕니다. 페이지 크기는 항상 일정합니다.

2. **주소 변환**
   - 세그멘테이션: 논리 주소는 세그먼트 번호와 오프셋으로 구성되며, 세그먼트 테이블을 통해 물리 주소로 변환됩니다.
   - 페이징: 논리 주소는 페이지 번호와 오프셋으로 구성되며, 페이지 테이블을 통해 물리 주소로 변환됩니다.

3. **단편화**
   - 세그멘테이션: 외부 단편화가 발생할 수 있습니다.
   - 페이징: 내부 단편화가 발생할 수 있습니다.

4. **사용 목적**
   - 세그멘테이션: 프로세스의 논리적 구조를 반영하는 데 적합합니다.
   - 페이징: 물리 메모리를 효율적으로 관리하는 데 적합합니다.

</details>

### OS-096
<details>
<summary>페이지와 프레임의 차이에 대해 설명해 주세요.</summary>
<hr>

1. **페이지**
   - 가상 메모리를 일정 크기로 나눈 단위입니다.
   - 논리 주소 공간에서의 개념입니다.

2. **프레임**
   - 물리 메모리를 일정 크기로 나눈 단위입니다.
   - 물리 주소 공간에서의 개념입니다.

3. **관계**
   - 페이지는 가상 메모리의 단위이고, 프레임은 물리 메모리의 단위입니다.
   - 페이지는 특정 프레임에 매핑되어 실행됩니다.

</details>

### OS-097
<details>
<summary>내부 단편화와, 외부 단편화에 대해 설명해 주세요.</summary>
<hr>

1. **내부 단편화**
   - 고정 크기 블록을 할당할 때, 블록 내부에 사용되지 않는 공간이 발생하는 현상입니다.
   - 예: 페이지 크기가 4KB이고 데이터가 3.5KB일 경우, 0.5KB가 낭비됩니다.

2. **외부 단편화**
   - 가변 크기 블록을 할당할 때, 사용 가능한 메모리가 충분하지만 연속된 공간이 부족해 할당이 불가능한 상태입니다.
   - 예: 10KB 공간이 있지만 연속된 8KB가 없을 때 메모리를 할당할 수 없습니다.

</details>

### OS-098
<details>
<summary>페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.</summary>
<hr>

1. **논리 주소 분리**
   - 논리 주소는 페이지 번호(Page Number)와 페이지 내 오프셋(Offset)으로 나뉩니다.

2. **페이지 테이블 조회**
   - 페이지 테이블에서 해당 페이지 번호에 대응하는 물리 프레임 번호(Frame Number)를 찾습니다.

3. **물리 주소 계산**
   - 계산식: `물리 주소 = (프레임 번호 * 페이지 크기) + 오프셋`

</details>

### OS-099
<details>
<summary>어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?</summary>
<hr>

1. **페이지 테이블 엔트리의 플래그**
   - 읽기/쓰기 권한 플래그를 확인합니다. 페이지가 읽기 전용인지, 쓰기가 가능한지를 판별합니다.

2. **메모리 보호 메커니즘**
   - 운영체제는 페이지 테이블을 통해 특정 주소 공간을 읽기 전용 또는 쓰기 가능으로 설정합니다.
   - 수정 권한이 없을 경우 Page Fault가 발생합니다.

</details>

### OS-100
<details>
<summary>32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?</summary>
<hr>

1. **주소 공간의 전체 크기**
   - 32비트 주소는 2의 32승 바이트(4GB)입니다.

2. **페이지 개수**
   - 전체 주소 공간을 페이지 크기로 나누면, `4GB / 1KB = 2의 22승 페이지`가 됩니다.

3. **페이지 테이블 크기**
   - 페이지 테이블 엔트리 크기를 4바이트로 가정하면, 페이지 테이블 크기는 `2의 22승 * 4바이트 = 16MB`입니다.

</details>

### OS-101
<details>
<summary>32비트 운영체제는 램을 최대 4G 까지 사용할 수 있습니다. 이 이유를 페이징과 연관 지어서 설명해 주세요.</summary>
<hr>

1. **32비트 주소 공간의 한계**
   - 32비트 주소는 2의 32승 개의 주소를 표현할 수 있으므로, 최대 4GB 메모리만 접근 가능합니다.

2. **페이징과의 연관성**
   - 운영체제는 가상 주소 공간과 물리 주소 공간을 페이징 기법으로 매핑합니다.
   - 물리 메모리가 4GB를 초과할 경우, 추가적인 주소를 매핑할 수 없습니다.

</details>

### OS-102
<details>
<summary>C/C++ 개발을 하게 되면 Segmentation Fault 라는 에러를 접할 수 있을텐데, 이 에러는 세그멘테이션/페이징과 어떤 관계가 있을까요?</summary>
<hr>

**Segmentation Fault**는 허용되지 않은 메모리 접근 시 발생하는 오류입니다. 

1. **세그멘테이션 관련**
   - 프로세스는 각 세그먼트(코드, 데이터, 스택)에 대한 권한이 정의되어 있습니다.
   - NULL 포인터 참조, 잘못된 세그먼트 접근 등이 Segmentation Fault를 유발합니다.

2. **페이징 관련**
   - 잘못된 가상 주소 접근 시 Page Fault가 발생하며, 이 문제가 해결되지 않으면 Segmentation Fault로 이어집니다.
   - 예: 읽기 전용 페이지에 쓰기를 시도하거나, 매핑되지 않은 페이지를 참조할 때 발생합니다.

</details>

## 📌 TLB

### OS-103
<details>
<summary>TLB는 무엇인가요?</summary>
<hr>

**TLB (Translation Lookaside Buffer)** 는 CPU 내부에 위치한 고속 캐시 메모리입니다. 가상 주소를 물리 주소로 변환하는 작업에서 페이지 테이블을 직접 조회하지 않고, 변환 정보를 빠르게 가져올 수 있도록 도와줍니다.  
주로 최근에 사용된 페이지의 변환 정보를 저장하며, 가상 메모리 관리 성능을 최적화합니다.

</details>

### OS-104
<details>
<summary>TLB를 쓰면 왜 빨라지나요?</summary>
<hr>

TLB를 사용하면 가상 주소를 물리 주소로 변환하는 속도가 빨라집니다. 

1. **페이지 테이블 접근 시간 단축**
   - 일반적으로 페이지 테이블 조회는 메모리 접근을 필요로 하며 상대적으로 느립니다.
   - TLB는 캐시 메모리이기 때문에 페이지 테이블을 조회하지 않아도 바로 변환 정보를 제공합니다.

2. **지역성(Locality) 활용** 
   - 대부분의 프로그램은 참조 지역성(Locality of Reference)을 따릅니다.
   - 최근 사용된 주소 변환 정보는 TLB에 저장되어 있으므로 반복적인 변환 요청을 빠르게 처리할 수 있습니다.

3. **MMU와의 협업** 
   - TLB는 MMU와 협력하여 주소 변환 속도를 극대화합니다.

</details>

### OS-105
<details>
<summary>MMU가 무엇인가요?</summary>
<hr>

**MMU (Memory Management Unit)** 는 CPU와 메모리 사이에서 주소 변환을 담당하는 하드웨어 장치입니다. 

1. **가상 주소 → 물리 주소 변환**
   - 프로세스가 사용하는 가상 주소를 물리 메모리 주소로 변환합니다.

2. **TLB와 협력**  
   - MMU는 변환 정보를 TLB에서 먼저 확인하며, TLB에 해당 정보가 없을 경우 페이지 테이블을 조회하여 변환을 수행합니다.

3. **메모리 보호 기능 제공**
   - 각 페이지에 대해 읽기, 쓰기, 실행 권한을 확인하고 위반 시 예외(Page Fault)를 발생시킵니다.

</details>

### OS-106
<details>
<summary>TLB와 MMU는 어디에 위치해 있나요?</summary>
<hr>

1. **TLB**
   - CPU 내부에 위치하며, 일반적으로 MMU와 매우 가까운 곳에서 작동합니다.  
   - 일부 시스템에서는 TLB가 MMU의 일부로 통합되어 있습니다.

2. **MMU**
   - CPU와 메모리 사이에 위치하며, 주소 변환 및 메모리 보호를 담당하는 하드웨어입니다.  
   - 현대 CPU에서는 MMU가 CPU 내부에 통합된 경우가 많습니다.

</details>

### OS-107
<details>
<summary>코어가 여러개라면, TLB는 어떻게 동기화 할 수 있을까요?</summary>
<hr>

프로세스가 공유 메모리를 사용할 때, 한 코어에서 페이지 테이블이 변경되면 다른 코어의 TLB도 이를 반영해야 합니다.

1. **TLB Shootdown**
   - 한 코어에서 페이지 테이블 엔트리가 수정되면, 다른 코어에 이 정보를 알립니다.
   - 각 코어는 변경된 엔트리에 해당하는 TLB 항목을 무효화합니다.

2. **TLB Flush**
   - 페이지 테이블 전역 변경 시, 모든 코어의 TLB를 완전히 비웁니다.
   - 성능이 저하될 수 있으므로 자주 수행되지 않도록 최적화가 필요합니다.

3. **Hardware Support**
   - 일부 최신 CPU는 하드웨어 레벨에서 TLB 동기화를 지원하여 성능 저하를 최소화합니다.

</details>

### OS-108
<details>
<summary>TLB 관점에서, Context Switching 발생 시 어떤 변화가 발생하는지 설명해 주세요.</summary>
<hr>

1. **TLB Invalidation**  
   - 프로세스가 전환되면, 이전 프로세스의 주소 변환 정보가 더 이상 유효하지 않습니다.
   - 따라서 대부분의 운영체제는 TLB를 비우거나 특정 엔트리를 무효화(TLB Flush)합니다.

2. **성능 저하**  
   - TLB가 비워진 상태에서 새 프로세스 실행 시, 페이지 테이블 조회가 증가하여 변환 속도가 느려질 수 있습니다.

3. **최적화 기술**
   - **Tagged TLB**  
     - 각 TLB 항목에 프로세스 ID(Tag)를 추가하여 여러 프로세스의 정보를 동시에 저장할 수 있습니다.
   - **ASID (Address Space Identifier)**  
     - 프로세스마다 고유한 식별자를 사용하여 TLB 무효화 없이 Context Switching을 지원합니다.

</details>

## 📌 동기화 구현

### OS-109
<details>
<summary>동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.</summary>
<hr>

1. **원자적 명령(Atomic Operations)**
   - **Test-and-Set**: 메모리의 특정 위치 값을 검사하고, 조건에 따라 값을 변경하는 명령입니다. 이는 원자적이므로 동시에 접근한 스레드가 서로 간섭하지 않습니다.
   - **Compare-and-Swap (CAS)**: 특정 메모리 값을 예상값과 비교하고, 일치할 경우 새로운 값으로 교체합니다. CAS는 락 없이 동기화를 구현하는 데 널리 사용됩니다.
   - **Fetch-and-Add**: 메모리의 값을 읽어오면서 동시에 덧셈 연산을 수행합니다. 병렬 환경에서 카운터를 관리할 때 유용합니다.

2. **메모리 배리어(Memory Barrier)**
   - CPU는 명령어 실행 순서를 최적화할 수 있는데, 이는 동기화 문제를 야기할 수 있습니다. 메모리 배리어는 명령어 재배치를 방지하여 메모리 연산의 순서를 보장합니다.
   - **Load Barrier**: 읽기 연산이 이전 쓰기 연산보다 먼저 실행되지 않도록 보장합니다.
   - **Store Barrier**: 쓰기 연산이 이전 쓰기 연산 이후에 실행되도록 보장합니다.

3. **하드웨어 락 지원**
   - CPU는 명령의 원자성을 보장하기 위해 **락 프리픽스**를 제공합니다. 예를 들어, x86 아키텍처의 `LOCK` 프리픽스는 메모리 접근을 다른 코어가 간섭하지 못하도록 잠금 상태로 만듭니다.

4. **캐시 일관성(Cache Coherence)**
   - 멀티코어 시스템에서는 각 코어가 별도의 캐시를 사용하므로, 데이터 일관성을 유지해야 합니다.
   - **MESI 프로토콜**: 데이터를 수정(Modified), 배타적(Exclusive), 공유(Shared), 무효(Invalid) 상태로 관리하여 캐시 일관성을 유지합니다. 이를 통해 여러 코어가 동일한 데이터를 읽고 쓰는 상황에서도 데이터 무결성을 보장합니다.

</details>

### OS-110
<details>
<summary>volatile 키워드는 어떤 의미가 있나요?</summary>
<hr>

`volatile` 키워드 컴파일러 최적화를 방지하는 키워드로, 하드웨어나 멀티스레드 환경에서 중요한 역할을 합니다.

1. **정의와 동작**
   - `volatile` 키워드가 선언된 변수는 **컴파일러가 최적화하지 않도록 보장**하며, 변수의 값을 항상 메인 메모리에서 읽고 메모리에 기록하게 만듭니다.
   - 이는 CPU 레지스터나 캐시를 통해 값이 저장되거나 재배치되는 것을 방지합니다.

2. **적용 예시**
   - **하드웨어 상태 확인**: 하드웨어 레지스터 값이 외부 요인에 의해 변경될 경우 `volatile`을 사용하여 값의 최신 상태를 읽습니다.
   - **멀티스레드 변수 접근**: 스레드 간 공유 변수에서 값을 캐시하지 않고 항상 최신 값을 사용해야 할 때 사용됩니다.
     
3. **제한점**
   - `volatile`은 변수 접근의 원자성을 보장하지 않습니다. 예를 들어, `volatile` 변수에 대해 증가 연산(++)을 수행할 경우, 읽기와 쓰기 사이에 다른 스레드가 값을 수정할 수 있습니다.
   - 따라서 **락, 원자적 연산**과 같은 추가 동기화 메커니즘이 필요합니다.

</details>

### OS-111
<details>
<summary>싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요?</summary>
<hr>

멀티코어 환경에서는 동기화 문제를 해결하기 위해 하드웨어와 소프트웨어 레벨에서 다양한 메커니즘이 결합됩니다.

1. **캐시 일관성(Cache Coherence)**
   - 각 코어가 데이터를 로컬 캐시에 저장하면서 수정할 수 있습니다. 데이터의 최신 상태를 모든 코어가 정확히 알기 위해 **캐시 일관성 프로토콜**이 사용됩니다.
   - **MESI 프로토콜**
     - 데이터는 네 가지 상태(Modified, Exclusive, Shared, Invalid)로 관리됩니다.
     - 한 코어가 데이터를 수정하면, 다른 코어의 해당 데이터는 `Invalid` 상태가 되어 최신 값을 다시 로드해야 합니다.
   - 이를 통해 캐시의 일관성을 유지하며 동기화 문제를 방지합니다.

2. **메모리 배리어(Memory Barriers)**
   - 멀티코어 환경에서 CPU는 명령어 재배치를 통해 효율성을 높이지만, 이는 동기화 문제를 유발할 수 있습니다.
   - **메모리 배리어**를 통해 코어 간 메모리 연산 순서를 강제하여 일관성을 유지합니다.
     - `mfence`, `lfence`, `sfence` 같은 명령어를 사용해 특정 연산의 실행 순서를 보장합니다.

3. **Inter-Processor Communication (IPC)**
   - 멀티코어 환경에서는 각 코어가 서로 데이터를 전달하거나 동기화할 필요가 있습니다.
   - **TLB Shootdown**: 한 코어가 페이지 테이블을 수정하면, 다른 코어의 TLB를 무효화하여 동기화 상태를 유지합니다.

4. **원자적 연산**
   - CAS(Compare-and-Swap)와 같은 원자적 명령은 멀티코어 환경에서도 데이터를 안전하게 수정할 수 있도록 보장합니다. 이는 하드웨어 수준에서 다른 코어의 간섭을 방지합니다.

5. **락 기반 동기화**:
   - **스핀락(Spinlock)**:
     - 멀티코어 환경에서 락을 확보하기 위해 바쁜 대기를 수행하는 락입니다.
   - **뮤텍스(Mutex)**:
     - 특정 스레드만 리소스를 사용할 수 있도록 보장하며, 락 확보 실패 시 대기 상태로 전환합니다.

</details>

## 📌 페이지 교체 알고리즘

### OS-112
<details>
<summary>페이지 교체 알고리즘에 대해 설명해 주세요.</summary>
<hr>

페이지 교체 알고리즘은 운영체제의 가상 메모리 관리에서, 페이지 부재(page fault)가 발생했을 때 어떤 페이지를 교체할지 결정하는 방법을 말합니다. 주요 목적은 페이지 부재를 최소화하여 효율적인 메모리 사용을 보장하는 것입니다.

</details>

### OS-113
<details>
<summary>LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?</summary>
<hr>

LRU 알고리즘은 "가장 오랫동안 사용되지 않은 페이지를 교체" 한다는 원칙을 따릅니다. 이를 통해 최근에 사용된 데이터는 더 오래 메모리에 남아 있게 됩니다.
지역성(Locality) 원칙: 최근에 사용된 페이지는 곧 다시 사용될 가능성이 높다는 가정(시간적 지역성)을 기반으로 합니다.

</details>

### OS-114
<details>
<summary>LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?</summary>
<hr>

해시맵 + 이중 연결 리스트를 조합한 방식으로 구현할 수 있습니다. 조회 시 해시맵을 사용하고 삽입,삭제 시에 연결리스트와 캐시를 구현해서 시간복잡도 O(1)의 성능으로 구현할 수 있습니다.

</details>

### OS-115
<details>
<summary>LRU 알고리즘의 단점을 설명해 주세요. 이를 해결할 수 있는 대안에 대해서도 설명해 주세요.</summary>
<hr>

LRU는 사용 이력을 유지해야 하므로, 추가적인 데이터 구조(해시맵, 연결 리스트 등)가 필요합니다. 또한 캐시 사용 패턴(워크로드)에 따라 성능 저하를 초래할 수 있습니다.

이 대안으로는 Clock, LFU, ARC와 같은 알고리즘으로 워크로드에 맞게 보완할 수 있습니다.

### **1. Clock (Second Chance)**  
- **동작 원리**: FIFO 방식에 사용 비트를 추가해 최근 사용 여부를 확인.  
- **장점**: 구현이 간단하고 메모리 오버헤드가 적음.  
- **단점**: LRU보다 정밀도가 떨어짐.  


### **2. LFU (Least Frequently Used)**  
- **동작 원리**: 참조 횟수가 적은 페이지를 교체.  
- **장점**: 자주 사용되지 않는 페이지를 효율적으로 제거.  
- **단점**: 최근 추가된 페이지가 교체될 가능성이 높음.  

### **3. ARC (Adaptive Replacement Cache)**  
- **동작 원리**: LRU와 LFU를 결합, 두 영역의 크기를 동적으로 조정.  
- **장점**: 다양한 워크로드에서 높은 성능.  
- **단점**: 구현이 복잡하고 메모리 오버헤드가 있음.  

</details>

## 📌 파일 시스템 및 I/O

### OS-116
<details>
<summary>File Descriptor와, File System에 에 대해 설명해 주세요.</summary>
<hr>

File Descriptor는 운영체제가 파일, 소켓 등을 식별하기 위해 사용하는 정수형 핸들입니다. 파일을 열면 운영체제가 디스크립터를 반환하며, 이를 통해 파일에 접근할 수 있습니다.
예) 0은 표준 입력, 1은 표준 출력, 2는 표준 오류

File System은 데이터를 디스크에 저장하고 관리하는 운영체제의 구성 요소입니다.
파일 시스템은 메타영역과 데이터영역으로 나뉘며, 메타영역은 파일의 이름, 경로, 데이터 위치, 메타데이터 등을 관리하고, 데이터영역은 파일의 실제 내용을 블록 단위로 저장합니다. 파일 시스템은 효율적인 디스크 관리와 신뢰성 있는 데이터 처리를 보장합니다.

</details>

### OS-117
<details>
<summary>I-Node가 무엇인가요?</summary>
<hr>

리눅스/유닉스 파일 시스템에서 파일의 메타데이터와 데이터 위치를 관리하는 중앙 관리 구조로, 디렉토리 엔트리가 I-Node 번호를 참조하는 방식입니다.
![image](./images/suhyen-OS-117(1).png)

</details>

### OS-118
<details>
<summary>프로그래밍 언어 상에서 제공하는 파일 관련 함수 (Python - open(), Java - BufferedReader/Writer 등)은, 파일을 어떤 방식으로 읽어들이나요?</summary>
<hr>

모든 프로그래밍 언어에서 파일 읽기는 운영체제의 파일 시스템을 통해 이루어집니다. 공통적으론 열기 -> 버퍼링 -> 읽기 -> 닫기 순으로 진행됩니다.
버퍼링 : 파일 데이터를 메모리에 직접 읽지 않고 버퍼를 통해 일정 크기씩 가져옴 -> 디스크와 메모리 간 속도 차이를 줄여 성능높이려고

### Python - open() : 직관적임
1. open() 함수로 파일을 열고 파일 디스크립터를 생성합니다.
2. 내부적으로 버퍼링을 사용해 파일을 일정 크기씩 읽어 메모리로 전달합니다.
3. 파일 데이터를 전체(read()), 줄 단위(readline()), 특정 크기(read(n))로 읽을 수 있습니다.

### Java  - BufferedReader : 스트림 기반
BufferedReader는 디스크에서 직접 데이터를 읽지 않고, 내부적으로 버퍼를 사용해 성능을 최적화합니다. 또한 파일 데이터를 줄 단위(readLine())로 읽거나, 스트림 형태로 처리할 수 있습니다. 때문에 BufferedReader는 대용량 파일에서도 효율적입니다.

</details>

## 📌 동기 & 비동기

### OS-119
<details>
<summary>동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.</summary>
<hr>

동기(Synchronous)

- 작업을 요청하면 작업이 완료될 때까지 기다립니다.
- 요청과 결과가 시간적으로 연결되어 있습니다.
- 예) 은행에서 순서대로 창구를 이용하는 상황.

비동기(Asynchronous)

- 작업을 요청하면 작업 완료를 기다리지 않고, 다른 작업을 수행할 수 있습니다.
- 요청과 결과가 시간적으로 독립적입니다.
- 예) 음식점에서 주문 후 호출벨을 받고, 음식을 받을 때까지 다른 일을 하는 상황.

</details>

### OS-120
<details>
<summary>그렇다면, 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?</summary>
<hr>

동기이면서 논블로킹은 의미가 있습니다. 호출자는 요청을 보낸 후 결과를 기다리지 않지만, 작업 상태를 계속 확인(polling)해야 하므로 동기적으로 작동합니다.

비동기이면서 블로킹인 경우는 의미가 없습니다. 비동기는 작업 완료를 기다리지 않는 것이 특징인데, 블로킹은 기다리는 개념이므로 충돌합니다. 그러나 결과를 강제로 기다려야 하는 경우는 제한적으로 사용될 수 있습니다.

블로킹(Blocking)

- 요청을 보낸 후 결과를 받을 때까지 대기합니다.
- 호출한 함수가 완료될 때까지 제어권이 호출자에게 반환되지 않습니다.
- 예) 전화를 걸고 상대방이 받을 때까지 기다리는 상황.

논블로킹(Non-Blocking)

- 요청을 보낸 후 결과를 기다리지 않고 즉시 반환합니다.
- 호출한 함수는 작업이 완료되지 않아도 제어권을 반환합니다.
- 예) 문자 메시지를 보내고 상대방이 답장을 보내는 동안 다른 일을 하는 상황.

</details>

### OS-121
<details>
<summary>I/O 멀티플렉싱에 대해 설명해 주세요.</summary>
<hr>

I/O 멀티플렉싱은 여러 I/O 작업을 동시에 처리하기 위한 기술입니다. 하나의 스레드가 여러 파일 디스크립터(소켓, 파일 등)를 감시하여 작업 가능 여부를 확인하고, 준비된 작업만 수행합니다.

예로 채팅 서버에서 여러 클라이언트가 연결될 때, 모든 연결을 하나씩 확인하는 대신, 준비된 연결만 처리합니다.

</details>

### OS-122
<details>
<summary>논블로킹 I/O를 수행한다고 하면, 그 결과를 어떻게 수신할 수 있나요?</summary>
<hr>

Polling
- 작업이 완료되었는지 주기적으로 확인합니다.
- 단점: 자원을 낭비할 수 있음.
- 예) 이메일 답장 왔나 계속 새로고침

Callback

- 작업이 완료되면 콜백 함수를 호출합니다.
- 장점: 효율적이고, 대기하지 않음.
- 예) 전화오면 알림음으로 알려주는 방식

Future/Promise:

- 결과를 비동기적으로 반환받을 수 있는 객체.
- 작업이 완료될 때까지 객체를 통해 상태를 확인하거나 결과를 가져옵니다.
- 예) 택배 추적 시스템에서 "배송 완료" 상태를 확인하는 방식.

Event-Driven 방식:

- 이벤트 루프를 통해 작업이 완료되면 알림을 받습니다.
- Node.js와 같은 이벤트 기반 환경에서 주로 사용

</details>
