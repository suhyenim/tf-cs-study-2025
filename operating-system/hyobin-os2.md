## 📌 IPC 및 스레드 안전성

### OS-064
IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요.

1. IPC(Inter-Process Communication): 서로 독립된 프로세스들이 데이터를 주고받거나 동기화하기 위해 사용하는 통신 메커니즘
- 프로세스는 메모리를 공유하지 않기 때문에, OS가 제공하는 IPC 도구를 통해서 데이터를 교환합니다.

2. IPC의 종류
- 파이프(Pipe) / Named Pipe(FIFO): 단방향 또는 이름 기반의 양방향 스트림을 전달
- 소켓: 네트워크 기반의 양방향 통신
- 공유 메모리(Shared Memory): 메모리를 공유하지만 별도의 동기화가 필요
- 메시지 큐: 커널이 큐 형태로 메시지를 전달
- 시그널: 프로세스에게 이벤트 알림
- 세마포어: 데이터 동기화용, 공유 자원 접근 제어
- 메모리 매핑 파일(MMAP): 파일을 메모리에 매핑해서 공유

3. 실제 사례
- Spring Boot + Redis Pub/Sub
  - 서버 A에서 Redis 채널에 메시지를 발행(Publish)
  - 서버 B가 해당 메시지를 구독(Subscribe)
  - → 서버 간 IPC를 Redis 메시지 큐 방식으로 수행

- Nginx + WAS 사이의 Unix Domain Socket
  - Nginx가 WAS(Spring Boot)로 요청을 보낼 때 Unix Socket을 사용하면 TCP보다 빠른 로컬 IPC가 가능

- Java에서 Shared Memory 기반의 IPC
  - Java NIO의 MappedByteBuffer는 파일을 메모리에 매핑해 다른 프로세스와 데이터를 공유
  - -> Shared Memory + Memory-mapped file 방식

- OS 레벨 프로세스 간 신호 전달
  - 프로세스가 SIGTERM을 받으면 종료 핸들러 실행
  - -> 시그널 기반 IPC의 전형적인 예

### 한줄 답변
- IPC는 프로세스 간 데이터를 주고받기 위한 통신 방식이며, 파이프/소켓/세마포어/메시지큐/시그널/공유메모리 등이 대표적이다.

### 꼬리 질문
- 소켓 기반 IPC에서 TCP와 UDP의 차이는 무엇인가?
- Java/Spring 환경에서 멀티 프로세스 간 동기화가 필요하다면 어떤 방법을 사용할 건지?
- IPC 사용이 Context Switching과는 어떤 관계가 있는지?

---

### OS-065
Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.

1. 개념
- 서로 다른 프로세스가 같은 메모리 영역을 공유해서 데이터를 **가장 빠르게** 교환할 수 있도록 하는 IPC 방식
- _데이터를 커널을 거쳐 복사하지 않아도 되기 때문에_ 빠르지만 **경쟁상태**와 **동기화 문제**가 발생할 수 있다.

2. 사례
- Java에서는 Shared Memory를 직접 쓰지 않는다. 대표적인 대안은 아래와 같다
  - Redis: In-Memory Data Store, 로그인 토큰이나 세션 정보를 공유해야 할 때 보통 Redis에 저장
  - Kafka/RabbitMQ: 메시지 큐, 여러 프로세스가 실시간 이벤트를 공유할 때 Kafka
  - RDBMS/NoSQL: 공용 데이터 저장소
  - HTTP REST API 기반의 서버 간 통신
  - 등등
  - => 이 기술들은 Shared Memory보다 느릴 수 있지만 동기화/락/일관성 문제를 프레임워크가 대신 책임져 준다는 점에서 훨씬 안정적이다

### 한줄 답변
- Shared Memory는 프로세스 간에 메모리를 공유할 때 가장 빠른 IPC 방식이지만, 동기화와 일관성 문제를 반드시 개발자가 직접 관리해야 한다.

### 꼬리질문
- Shared Memory IPC 방식을 사용했을 때의 문제점과 해결법은?
- Message Queue 방식이랑 비교했을 때 어떤 상황에서 Shared Memory를 사용하면 안 되는지?
- Java에서 Shared Memory의 대안이 되는 기술이 뭔지?

---

### OS-066
메시지 큐는 단방향이라고 할 수 있나요?

- 메시지 큐 자체는 기본적으로 단방향 통신 모델이 맞지만, 시스템 전체로 보면 양방향 통신을 구성할 수 있는 방식이다.
- Queue 구조의 특성상 Producer -> Queue -> Consumer 로 한 큐를 기준으로 보면, 반드시 한 방향으로만 흐름
- 하지만 요청용 큐/응답용 큐를 따로 만들어두면 양방향 구성이 가능
  - 이를 RPC (Remote Procedure Call)라고 함
- -> 즉 모델은 단방향, 시스템은 양방향 구성 가능

### 한줄 답변
- 메시지 큐는 기본적으로 단방향 구조이지만, 요청/응답 큐를 조합하면 양방향 통신도 구현할 수 있다.

---

### OS-067
Thread Safe 하다는 것은 어떤 의미인가요?

1. 개념
- 여러 스레드가 동시에 같은 코드나 데이터에 접근해도 데이터가 깨지지 않고 항상 올바른 결과가 보장되는 상태를 말한다.
- Thread Safe의 보장 조건
  - 경쟁 조건(race condition)을 방지
  - 원자성 보장
  - 가시성 보장
  - 동기화/락을 이용한 순서 보장

2. 사례
- ArrayList vs Vector
  - ArrayList: Thread Safe 아님 -> 여러 스레드가 동시에 add()하면 데이터 깨짐
  - Vector: 모든 메서드에 synchronized -> Thread Safe하지만 락이 많이 걸려 느림

- Spring에서 Singleton 빈을 Thread Safe하게 관리해야 하는 이유
  - Spring Bean은 기본적으로 싱글톤이기 때문에 여러 요청이 동시에 같은 객체를 사용

- DB Connection Pool (HikariCP)
  - Connection Pool 객체는 여러 스레드가 동시에 접근하지만 내부적으로 synchronized + CAS + 락 전략으로 Thread Safe하게 커넥션을 관리함.

### 한줄 답변
- Thread Safe란 여러 스레드가 동시에 같은 자원에 접근하더라도 데이터 무결성과 일관성이 항상 유지되는 상태이다.

### 꼬리 질문
- 싱글톤 빈에서 Thread Safety를 깨는 대표적인 실수는 무엇인지?

---

### OS-068
Thread Safe 를 보장하기 위해 어떤 방법을 사용할 수 있나요?

- synchronized(모니터 락) 사용
- ReentrantLock 같은 명시적 락 사용
  ` lock.lock();
    try {
    count++;
    } finally {
    lock.unlock();
    }`
- AtomicInteger 등 Atomic 클래스(CAS 기반)

- volatile을 통한 가시성 보장
  - 가시성은 보장되나 원자성은 보장 X

- **불변 객체(Immutable Object) 사용**
  - Java String
  - Spring에서 대부분의 설정 객체

- ThreadLocal로 스레드별 독립 데이터 사용
  - 요청마다 새로운 인스턴스를 만드는 대신 ThreadLocal 내부에 저장
  - → 스레드 간 공유를 아예 제거하여 Thread Safe 달성

- 공유 상태(stateless) 제거하는 설계
  - 최고의 전략은 공유 데이터를 없애는 것
  - `@Service
    public class CalcService {
    public int add(int a, int b) { return a + b; }
    }`
  - 내부 저장 상태가 없어서 동시성 문제가 발생할 여지 없음
- Concurrent 자료구조 사용 (ConcurrentHashMap 등)


### 한줄 답변
- Thread Safe를 보장하기 위한 최고의 전략은 무상태 설계라고 생각. 공유할 데이터 자체가 없으면 동시성 문제가 발생한 여지가 없기 때문
- 이외에도 불변객체나 synchronized/락 전략, volatile, ThreadLocal 등의 방식이 있다.

---

### OS-069
Peterson's Algorithm 이 무엇이며, 한계점에 대해 설명해 주세요. (고전 동기화 알고리즘)

- 두 개의 스레드(또는 프로세스)가 하나의 임계 구역에 동시에 진입하지 못하게 보장하는 고전적인 상호배제 알고리즘
- 핵심 아이디어는 두 변수만으로 동작
  - flag[i]: 임계구역에 들어갈 거임
  - turn: 마지막으로 임계구역 진입을 양보한 스레드 번호
- Peterson 알고리즘은 다음 3가지를 모두 보장
  - 상호배제(Mutual Exclusion)
  - Progress(진행 조건)
  - Bounded Waiting(유한 대기)

- 그러나 현실에서는 이 알고리즘은 사용하지 않음!!
  - Peterson은 메모리가 순차적 일관성을 가진다고 가정하지만 현대 CPU는 다음의 최적화를 사용: Instruction Reordering, CPU 캐시, Store Buffer
  - Busy Waiting(Spin Lock)이기 때문에 비효율적 - 상대 스레드가 끝날 때까지 무한 루프를 도는 방식(CPU를 계속 태움🔥)
  - Peterson은 2-thread 전용 알고리즘 - N개 스레드를 위한 확장판이 있긴 하나 사용 X
  - JVM/Spring에서는 락이나 CAS로 간단히 해결 가능

### 한줄 답변
- Peterson 알고리즘은 두 스레드 간 **상호 배제를 보장**하는 고전적 알고리즘이지만, 현대 CPU의 메모리 모델·바쁜 대기·2스레드 제한 때문에 실제로는 사용할 수 없다.

---

### OS-070
Race Condition 이 무엇인가요?

1. 개념
- 여러 스레드가 동시에 공유 자원을 읽거나 수정할 때, 실행 순서에 따라 결과가 달라지는 문제를 의미한다
- 즉 누가 먼저 실행되느냐에 따라서 프로그램 결과가 달라지는 상태를 말하다.
- 발생 조건
  - 여러 스레드가 같은 공유 자원에 접근하는 경우
  - 최소 하나 이상이 쓰기 연산을 수행하는 경우

2. 사례
- Spring Boot Singleton Bean 내부 상태 공유
  - Spring Bean은 기본적으로 singleton이므로 여러 요청(thread)이 동일 객체에 접근
  - 운영 환경에서 카운터, 캐시, map 같은 공유 상태가 있을 때 흔히 발생

- ConcurrentHashMap을 사용하지 않고 HashMap 공유
  - 멀티스레드 환경에서 HashMap을 공유하면 re-hash 과정에서 무한 루프가 발생하기도 함 
  - -> Thread Safety 없는 자료구조를 공유하면 Race Condition으로 구조 자체가 깨짐

- DB Update에서도 Race Condition 발생
  - 두 요청이 동시에 같은 row를 update할 때 Optimistic Lock이 없다면 결과가 덮어씌워진다

### 한줄 답변
- Race Condition은 여러 스레드가 동시에 공유 데이터를 조작할 때 실행 순서에 따라 결과가 달라지는 문제이다

### 꼬리질문
- Race Condition을 방지하기 위한 기법으로는 무엇이 있는지?

---

### OS-071
Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?

- 반드시 락이 필요한 것은 아니다. 락 없이도 Thread Safety를 보장하는 다양한 방법이 존재한다
  - Atomic 기반(CAS 기반) 연산 사용
  - Immutable(불변) 객체 사용
  - ThreadLocal로 스레드별 독립 데이터 유지
  - Stateless 디자인(공유 상태 자체를 제거)
  - Copy-on-write 전략
  - 메시지 패싱 방식
- 락은 직관적이고 강력하지만, 성능 저하/교착상태/컨텍스트 스위칭 등의 부작용 때문에 최선의 선택이 아닐 수 있다

### 한줄 답변
- Thread Safe는 락 없이도 CAS, 불변 객체, ThreadLocal, Stateless 설계 등으로 구현 가능하다.

### 꼬리질문
- 불변 객체는 왜 Thread Safe한가?
- Stateless 설계가 가장 이상적이라고 하는 이유는?

---

### OS-072
Thread Pool, Monitor, Fork-Join에 대해 설명해 주세요.

1. Thread Pool
- 미리 여러 스레드를 만들어놓고, 작업이 들어올 때 스레드를 재사용하는 방식의 스레드 관리 기법
- 스레드를 매번 생성하고 파괴하면 그 비용이 크기 때문에 풀에서 꺼내 쓰면 성능이랑 안정성을 확보할 수 있다.
- 특징: 스레드 재사용 + 작업 큐 기반 + 동시성 제어 + 최대 스레드 수 제한 + 폭주 방지 - 시스템 보호

2. 모니터
- 상호배제(Mutex) + 조건 변수(Condition Variable)를 묶어 제공하는 동기화 메커니즘
- Java의 synchronized가 대표적인 Monitor 개념의 실현체
- 제공: 락을 통한 임계 구역 보호, wait/notify로 조건이 충족될 때까지 대기/깨우기, 한 번에 하나의 스레드만 접근(Mutual Exclusion)

3. Fork-Join
- 큰 작업을 작은 작업으로 쪼개고(Fork), 결과를 합치는(Join) 병렬 처리 프레임워크
- **분할 정복 기반의 프레임워크**
- Java의 ForkJoinPoll이 대표 구현체

### 한줄 답변
- Thread Pool은 스레드를 재사용하는 병렬 처리 기법, Monitor는 락+조건변수로 상호 배제를 구현하는 동기화 도구, Fork-Join은 큰 작업을 분할하고 병렬로 처리하는 분할정복 기반 프레임워크이다.

---

### OS-073
Thread Pool을 사용한다고 가정하면, 어떤 기준으로 스레드의 수를 결정할 것인가요? (중요🚨)

1. 개념
- Thread Pool의 스레드 수는 작업이 CPU-bound인지 I/O bound인지에 따라서 다르게 설정한다.
  - CPU-bound: 코어 개수 정도로 설정(N or N+1)
    - Spring Batch에서 CPU를 계속 쓰는 연산
    - 스레드 개수를 늘려도 CPU가 포화되어 오히려 더 느려짐
  - I/O-bound: I/O 대기 시간이 많기 때문에 더 많은 스레드가 필요 -> 대략 `CPU 코어 수 X (1 + I/O 대기 시간 / 작업 시간)`
    - 웹서버(스프링)에서 외부 API 호출이 많다면 I/O 대기가 길어짐
    - 그래서 CPU 8코어 서버에서 I/O-heavy 서비스는 50~200 스레드 사용
    - `Spring Boot 기본값: core=8, max=200 정도`

- 운영 환경에서는 다음 요소들도 고려해야 한다
  - CPU 사용률
  - 메모리 여유
  - 외부 API/DB 응답 시간
  - 큐 용량 및 처리량(TPS)
  - 스레드 컨텍스트 스위칭 비용

2. 사례
- 실제 운영 환경 – HikariCP Connection Pool 영향
  - DB Connection Pool이 30이라면, 비동기 스레드를 100개 만들면 → 결국 DB 커넥션이 부족해서 줄 서는 상황 발생. 
  - 따라서 스레드 수는 DB pool과도 맞춰야 함:
    - HikariCP maxPoolSize = 30 
    - Thread Pool maxSize = 40~60 정도로 제한

### 한줄 답변
- 스레드 수는 CPU-bound는 코어 수, I/O-bound는 코어 수 × 대기비율로 결정하며, DB pool·대기 시간·TPS·메모리까지 고려해야 한다.

---

### OS-074
어떤 데이터를 정렬 하려고 합니다. 어떤 방식의 전략을 사용하는 것이 가장 안전하면서도 좋은 성능을 낼 수 있을까요?

- 입력된 데이터의 특성을 모르는 상황에서는 Timsort 기반의 정렬을 사용하는 것이 가장 안전하면서 좋은 성능을 낸다.
- TimSort
  - 합병 정렬(Merge Sort)과 삽입 정렬(Insertion Sort)의 장점만 섞은 하이브리드 알고리즘
  - 최악 시간 복잡도를 `O(N log N)`으로 보장 + 실무 데이터에서 매우 빠른 성능 + 안정성 보장을 갖춘 하이브리드 정렬 알고리즘
  - 따라서 대부분의 현대 언어는 내부 정렬에서 Timsort 사용
    - Java의 Arrays.sort() (객체 정렬)
      - Primitive 타입은 QuickSort 사용

### 한줄 답변
- 입력 특성을 모르는 일반적인 환경에서는 안정적이고 최악 O(n log n)을 보장하는 Timsort가 가장 안전하고 실무 성능도 뛰어난 정렬 전략이다.

### 꼬리질문
- TimSort란 어떤 알고리즘인지?
  - 삽입 정렬의 '작은 데이터 처리 효율성'과 합병 정렬의 '안정적인 분할 정복'을 결합하여, 실무 데이터(부분 정렬된 데이터)에서 최적의 성능을 내도록 설계된 알고리즘이다

---

## 📌 캐시

### OS-075
캐시란 무엇이며 캐시 메모리 및 메모리 계층성에 대해 설명해 주세요.

1. 개념
- 캐시는 느린 저장 장치에 반복적으로 접근할 때 성능을 높이기 위해서 더 빠른 저장 장치에 데이터를 임시로 저장해두는 기술이다.
- CPU 관점에서는 메모리 접근 병목을 줄이기 위한 고속 메모리를 의미하고, 애플리케이션 관점에서는 DB나 API 호출을 줄이기 위한 중간 저장소 역할을 한다.

- 캐시 메모리는 CPU와 메모리 사이의 병목을 줄이기 위해서 사용되는 초고속 SRAM 기반 메모리이며, 보통 3계층 구조(L1, L2, L3)를 가진다.

- 메모리 계층성은 위부터 `빠르고 비싼 것 -> 느리고 싸고 큰 것`의 구조로 계층화된 메모리 시스템이다.
- 자주 쓰이는 데이터는 빠른 계층인 위쪽에, 덜 쓰이는 데이터는 아래 계층에 둬서 전체의 비용과 성능을 최적화하는 구조이다.

2. 사례
- DB에 자주 접근하면 느리기 때문에 Redis에 캐시
  - DB는 느린 계층, Redis는 빠른 계층 => 메모리 계층성의 애플리케이션 버전

### 한줄 답변
- 캐시는 느린 저장소 접근을 줄이기 위해 빠른 저장소에 데이터를 임시 저장하는 기술이며, CPU는 L1~L3 캐시 기반의 메모리 계층성을 통해 전체 성능을 최적화한다.

---

### OS-076
캐시 메모리는 어디에 위치해 있나요?

1. 개념
- 캐시 메모리는 CPU와 메인 메모리(DRAM) 사이에 위치하며 대부분은 **CPU 칩 내부(온ON칩)**에 포함되어 있다.
- L1/L2 캐시는 CPU 코어 내부에 있고, L3는 CPU 다이(die) 안에서 여러 코어가 공유하는 구조이다.
- **즉 캐시는 메인 메모리보다 CPU에 더 가까운 위치에 존재하는 고속 SRAM 영역이다.**

- CPU 캐시가 온칩이기 때문에 극단적으로 빠른 이유
- CPU가 메모리를 읽을 때:
  - L1 Cache hit → 수 ns
  - L2 Cache hit → 수~수십 ns
  - L3 Cache hit → 수십 ns
  - DRAM 메모리 접근 → 약 100ns~200ns
  - SSD 접근 → 수십 μs
  - 네트워크 접근 → ms
  - → 캐시가 CPU 바로 옆(또는 내부)에 있어야 하는 이유: 레벨이 내려갈수록 대기 시간이 폭발적으로 증가함.

### 한줄 답변
- 캐시 메모리는 CPU 내부 또는 바로 옆에 위치하며(L1/L2/L3), CPU와 메인 메모리 사이에서 속도 차이를 줄이는 초고속 메모리 계층이다.

### 꼬리질문
- CPU 캐시가 빠른 이유는?
- 왜 캐시는 DRAM이 아닌 SRAM으로 만들어지는가?
- DRAM과 SRAM은 각각 무엇이고 차이는?


---

### OS-077
L1, L2 캐시에 대해 설명해 주세요.

1. 개념
- L1 캐시: CPU 코어 내부에 있는 가장 작고 빠른 캐시
  - 접근 속도: 수 ns 미만
  - 용량: 32KB~64KB
- L2 캐시: L1보다 좀 더 크고 느린 캐시, 대부분 코어별로 존재, L1에서 미스가 나면 L2에서 탐색
  - 접근 속도: L1보다 3~4배 느림
  - 용량: 256KB~1MB

- 이러한 계층형 구조를 통해 CPU는 메모리에의 평균 접근 시간을 단축할 수 있다.

2. 사례
- 배열 연속 접근이 빠른 이유 -> L1/L2 캐시 지역성(Locality) 덕분
  - `for (int i = 0; i < arr.length; i++) {
    sum += arr[i];
    }`
  - 배열이 연속된 메모리에 있고 캐시 라인 단위로 미리 불러오기 때문에 L1 캐시에 Hit 확률이 매우 높다
  - -> CPU 성능이 눈에 띄게 향상됨

- 그러나 랜덤 접근은 더 느리다 -> L1/L2 캐시 미스 증가
  - `sum += arr[randomIndexes[i]];`
  - 랜덤 접근은 지역성이 없어서 CPU가 L1 -> L2 -> L3 -> DRAM까지 내려가야 한다 -> 성능 저하
  - 그래서 실제 대규모 트래픽 환경에서 메모리 접근 패턴을 고려한 자료구조 선택이 중요한 것(HashMap vs ArrayList)

### 한줄 답변
- L1은 가장 빠르고 작은 CPU 코어 내부 캐시이며, L2는 L1보다 느리지만 용량이 큰 2차 캐시로, 둘의 계층 구조를 통해 평균 메모리 접근 시간을 최소화한다.

### 꼬리질문
- 캐시는 왜 이런 계층형 구조를 사용하는가?
- 랜덤 접근의 속도를 확보하기 위해 HashMap vs ArrayList 중 어떤 자료구조를 사용해야 하는가?
- Spring 고성능 서버에서 HashMap 기반 캐시를 조심해야 하는 이유는?


---

### OS-078
캐시에 올라오는 데이터는 어떻게 관리되나요?

- 캐시에 올라오는 데이터는 교체 정책과 일관성 규칙에 따라서 자동으로 관리된다
- 캐시는 용량이 작기 때문에 어떤 데이터를 유지하고, 어떤 데이터를 제거할지 결정해야 하며, 이를 운영체제 혹은 캐시 시스템이 관리한다.

- 관리방식
  1. 캐시 교체 정책(Replacement Policy)
  - 공간이 없을 때 어떤 데이터를 버릴지 결정하는 전략
  - 대표 정책
    - LRU(Least Recently Used): 가장 오래 안 쓰인 데이터를 제거
    - LFU(Least Frequently Used): 사용 빈도가 낮은 데이터를 제거
    - FIFO: 먼저 들어온 데이터를 먼저 제거
    - Random: 임의 제거
    
  2. 캐시 라인 단위 관리
  - CPU 캐시는 데이터를 1바이트씩 저장하지 않고, 보통 64바이트 단위의 캐시 라인으로 저장함
  - 데이터 접근 시 인접 데이터까지 한 번에 가져옴(공간 지역성)
  - 캐시 히트/미스 관리의 기본 단위
  
  3. 캐시 일관성 관리
  - 여러 코어가 있는 CPU에서는 캐시 데이터가 서로 다를 수 있음
  - 이를 해결하는 프로토콜: MESI 프로토콜
  - 여러 캐시 간 동일한 데이터의 최신 상태를 보장하는 메커니즘
  
  4. 캐시 쓰기 정책
  - 메모리에 쓸 때 어떤 방식으로 업데이트할지 결정
  - Write-through: 캐시에 쓰면 즉시 DRAM에도 씀 -> 안정성 높음, 성능 낮음
  - Write-back: 캐시에만 쓰고 나중에 메모리에 반영 -> 성능 좋고 복잡

2. 사례
- 캐시 일관성 문제로 인한 성능 저하 (False Sharing)
  - 두 스레드가 서로 다른 변수를 쓰는데 그 변수가 같은 캐시 라인에 있을 경우
  - 계속 서로 Invalidate -> 성능 급락
  - Spring 에서 멀티스테륻 환경에서 발생하는 대표 병목

### 한줄 답변
- 캐시 데이터는 교체 정책(LRU·LFU), Cache Line 단위, 일관성 프로토콜(MESI), 그리고 쓰기 정책(write-through/back)에 따라 자동으로 관리된다.

### 꼬리 질문
- LRU가 LFU보다 더 적합한 경우는 언제인지?
- 캐시 라인의 필요성은?
- Redis 캐시에서 만료(TTL)와 교체 정책은 어떻게 다른지?

---

### OS-079
캐시간의 동기화는 어떻게 이루어지나요?

1. 개념
- 캐시 간의 동기화(Cache Coherence)는 여러 CPU 코어가 같은 메모리 주소를 캐시에 저장할 때, 각 캐시의 값이 서로 일치하도록 유지하는 메커니즘
- 여러 코어가 동시에 같은 데이터를 읽고 쓰기 때문에 각 캐시가 제멋대로 값을 가지면 시스템 전체가 오작동한다.
- 이를 해결하기 위해 CPU는 하드웨어 차원에서 일관성 프로토콜을 적용해 캐시들의 값을 자동으로 동기화한다.

- 대표 프로토콜: MESI 프로토콜
  - 캐시 라인의 상태를 4가지로 관리
  - M(Modified): 캐시에만 반영된 최신 값(메모리와 불일치)
  - E(Exclusive): 내가 가진 값이 최신이며 다른 코어에는 없음
  - S(Shared): 여러 코어가 동일 값을 공유
  - I(Invalid): 다른 코어가 값을 변경했으므로 내 값은 폐기

2. 사례
- 두 스레드가 같은 변수 x를 읽고/쓰는 상황
  - CPU Core 1: x=10;
  - CPU Core 2: print(x);
  - 1) Core 1이 x를 변경 -> 캐시 라인은 Modified(M)
  - 2) 다른 코어의 동일 주소 캐시는 Invalid(I) 상태로 변경
  - 3) Core 2가 x를 읽으려고 하면 -> 자신의 캐시가 invalid이므로 Core 1 또는 메모리에서 최신 값을 가져와 Shared(S) 상태로 갱신
  - 4) 값이 자동으로 동기화

### 한줄 답변
- 캐시간 동기화는 MESI 같은 **캐시 일관성 프로토콜**을 통해 CPU들이 서로의 읽기/쓰기 이벤트를 감지하고 캐시 라인을 invalidate·update 하는 방식으로 자동 관리된다.

---

### OS-080
캐시 메모리의 Mapping 방식에 대해 설명해 주세요.

1. 개념
- 캐시 메모리의 매핑 방식은 메인 메모리의 특정 주소가 캐시의 어느 위치에 저장될 것인지를 결정하는 방식이다
- 캐시는 작고 빠르기 때문에, 어떤 메모리 블록을 어디에 저장할지 결정하는 전략이 필요하다

2. 대표적인 매핑 방식
- Direct-Mapped Cache (직접 매핑)
  - 메모리 블록 1개 → 특정 캐시 라인 1개에만 저장 가능
  - 구조 단순하고 빠름, but 충돌 잦음
  - -> 같은 라인을 두 데이터가 계속 대체하면 thrashing 발생

- Fully Associative Cache (완전 연관)
  - 메모리 블록 1개 → 캐시 어디든 저장 가능
  - 가장 유연해서 Conflict Miss 거의 없음
  - 보통 CPU 캐시에는 안 쓰이고 TLB 등 작은 캐시에 사용

- Set Associative Cache (N-웨이 집합 연관)
  - 메모리를 여러 Set으로 나누고, 각 Set 안에서만 아무 라인에 저장 가능
    (예: 4-way → Set 안에는 4개의 Cache Line 존재)
  - 대부분의 L1/L2/L3 캐시가 N-way Set Associative 방식

### 한줄 답변
- 캐시 매핑 방식은 Direct-Mapped·Fully Associative·Set Associative가 있으며, 현대 CPU는 대부분 성능과 충돌률 균형이 좋은 Set Associative 방식을 사용한다.

---

### OS-081
캐시의 지역성에 대해 설명해 주세요.

1. 시간 지역성(Temporal Locality)
- 최근 접근한 데이터는 곧 다시 접근할 가능성이 높다 -> CPU는 최근 사용 데이터를 캐시에 남겨두면 성능 향상
- ex. 반복문에서 같은 변수를 게속 읽은 경우, Java에서 HotSpot JIT가 자주 쓰는 메서드를 최적화하는 이유

2. 공간 지역성(Spatial Locality)
- 현재 접근한 주소 근처의 데이터도 곧 접근할 가능성이 높다 -> CPU는 한 번 접근 시 인접 데이터를 캐시 라인(64 바이트) 단위로 가져온다
- ex. 배열을 순차적으로 탐색할 때, 객체 필드가 같은 메모리 근처에 배치되어 있을 때

3. 순차 지역성(Sequential Locality)
- 프로그램이 메모리를 순서대로 읽는 경향(공간 지역성의 확장 개념)
- ex. 반복문에서 i=0 -> i=1 -> i=2 ...


### 한줄 답변
- 캐시의 지역성(Locality)이란, 프로그램이 메모리를 접근할 때 특정 패턴이 반복되는 특성을 말하며, 이 특성을 활용해 캐시 hit를 극대화하는 개념
- 캐시 지역성은 프로그램이 최근 접근(시간) 또는 가까운 주소(공간)의 데이터를 반복적으로 사용하는 특성으로, 캐시가 성능을 극적으로 향상시키는 핵심 원리이다.

### 꼬리 질문
- HashMap 대신 ArrayList를 쓰면 캐시 효율이 좋아지는 이유는?
- Java의 LinkedList가 느린 이유를 지역성 관점에서 설명한다면?
- 캐시 지역성을 최대로 살리기 위한 코드 최적화 기법은?

---

### OS-082
캐시의 지역성을 기반으로, 이차원 배열을 가로/세로로 탐색했을 때의 성능 차이에 대해 설명해 주세요.

- `arr[0][0], arr[0][1], arr[0][2], ...`
- `arr[1][0], arr[1][1], arr[1][2], ...`
- 같은 행(row)은 메모리에 연속적으로 존재
- 같은 열(column)은 메모리에서 멀리 떨어져 있음
- 그래서 행 우선 탐색은 메모리 순서랑 접근 순서가 일치(for i + for j)해서 CPU가 한 번 64바이트 캐시 라인을 가져올 때
- 연속된 arr[i][j], arr[i][j+1],... 이 모두 포함된다
- 그래서 캐시 히트율이 높아 매우 빠름

- 그러나 열 우선 탐색은 메모리 순서와 맞지 않는다
- 각 접근마다 서로 멀리 떨어진 메모리 주소 접근 -> 캐시 라인 사용률이 떨어지고, 캐시 미스가 증가 -> DRAM 접근 증가 -> 성능 저하

### 한줄 답변
- 2차원 배열은 메모리에 ‘행(Row)’ 단위로 연속 저장되기 때문에, 가로(행 우선) 탐색은 공간적 지역성이 좋아 캐시 히트가 높고, 세로(열 우선) 탐색은 메모리 건너뛰기가 발생해 캐시 미스가 증가하여 성능이 떨어진다

### 꼬리질문
- LinkedList가 ArrayList보다 캐시 효율이 낮은 이유는 무엇인지?

---

### OS-083
캐시의 공간 지역성은 어떻게 구현될 수 있을까요? (힌트: 캐시는 어떤 단위로 저장되고 관리될까요?)

- 캐시의 공간 지역성이 구현되는 핵심 메커니즘은 캐시가 메모리를 캐시 라인 단위로 가져오는 것
- 캐시는 1바이트 단위로 메모리를 가져오지 않고 64바이트의 캐시 라인을 가져온다
- 즉 미리 주변 데이터까지 챙겨놓는 것!! 

- why?
  - 프로그램은 연속된 메모리를 자주 사용하기 때문이다 - 배열 순회

### 한줄 답변
- 캐시는 메모리를 캐시 라인 단위로 블록화해서 저장하기 때문에 한 주소를 접근하면 주변 데이터도 함께 로딩되어 공간 지역성이 구현된다.

---

## 📌 메모리 할당

### OS-084
연속할당 방식 세 가지를 설명해주세요. (first-fit, best-fit, worst-fit)

1. 개념
- 연속 메모리 할당: 프로세스를 어느 빈 공간에 넣을지 결정하는 알고리즘

- 최초 적합(First-fit)
  - 메모리의 처음부터 검색해서 프로세스가 들어갈 수 있는 첫 번째 공간에 바로 할당하는 방식
  - 검색 속도가 가장 빠르지만, 메모리 앞부분에 할당이 집중되어 조각(Fragmentation)이 생길 수 있다
  - ex. DBCP (Database Connection Pool): 스프링 부트에서 HikariCP가 커넥션을 빌려줄 때, 사용 가능한 커넥션 리스트를 순회하다가 가장 먼저 발견된 유휴(Idle) 커넥션을 즉시 반환하는 방식과 유사

- 최적 적합 (Best-fit)
  - 모든 가용 공간을 검색하여 프로세스 크기와 가장 차이가 적은 공간에 할당
  - 큰 공간을 아낄 수 있지만, 검색 시간이 오래 걸리고 아주 작은 자투리 공간(외부 단편화)가 발생할 수 있다

- 최악 적합(Worst-fit)
  - 모든 가용 공간을 검색하여 가장 큰 공간에 할당
  - 할당 후 남은 공간도 여전히 크기 때문에 다른 프로세스가 사용할 확률이 높다. 그러나 검색 비용이 든다.
  - ex. 로드 밸런서(Load Balancer): 메모리 할당은 아니지만, 트래픽 분산 시 가장 리소스 여유가 많은(가장 큰) 서버에 무거운 작업을 우선 배정하는 'Least Connection'이나 'Weighted Round Robin' 방식

### 한줄 답변
- First-fit은 속도 효율, Best-fit은 공간 효율을 추구하지만 미세한 파편화가 발생하며, Worst-fit은 할당 후 남은 공간의 활용성을 높이려는 전략입니다.

### 꼬리 질문
- 외부 단편화란? 어떻게 해결할 수 있는지?
- 내부 단편화란? 어떻게 해결할 수 있는지?
- 실제 현대 OS는 어떤 방식을 사용하는지?💡

---

### OS-085
worst-fit 은 언제 사용할 수 있을까요?

- 가장 큰 공간을 쪼개서 할당하고 나면, 남게 되는 공간도 여전히 크다는 것이 핵심이다.
1) 요청되는 메모리 크기가 불규칙적일 때:
   - 작은 프로세스와 큰 프로세스가 번갈아 가며 들어오는 환경
   - Best-fit을 쓰면 너무 작은 자투리 공간(모래알 같은 공간)이 많이 생겨서, 나중에 들어오는 중간 크기의 프로세스를 수용하지 못할 수도 
   - Worst-fit을 쓰면 남은 공간이 충분히 크므로, 이후에 들어올 프로세스도 그 남은 공간에 수용될 가능성이 높다

2) 외부 단편화(External Fragmentation)를 줄이고 싶을 때:
   - 미세한 조각(Unusable holes)이 발생하는 것을 방지하여, 메모리 공간을 '큼직하게' 유지하고 싶을 때 사용합니다.


### 한줄 답변
- Worst-fit은 할당 후 남은 공간도 다른 프로세스가 활용할 수 있도록 큼직하게 유지하여, 미세한 외부 단편화 발생을 줄이고자 할 때 사용합니다

### 꼬리 질문
- 실제 OS에서 Worst-fit을 잘 안 쓰는 이유는?

---

### OS-086
성능이 가장 좋은 알고리즘은 무엇일까요?

- 시간 효율성 (Speed) - 1등
  - First-fit은 메모리 공간을 찾을 때, 적절한 공간이 나오면 즉시 검색을 멈추고 할당
  - 반면 Best-fit과 Worst-fit은 리스트 전체를 끝까지 뒤져야 하므로 $O(N)$의 시간이 무조건 소요된다.

- 공간 효율성 (Space)
  - 직관적으로는 Best-fit이 공간을 제일 아낄 것 같지만, 실제 시뮬레이션 결과 First-fit과 Best-fit의 공간 효율성은 비슷하게 나타난다.
  - 오히려 Best-fit은 너무 작은 자투리 공간(활용 불가능한 미세 조각)을 많이 만들어내어 나중에 정리(Compaction) 비용을 발생시킬 위험이 크다.

### 한줄 답변
- 시간과 공간 효율성을 종합적으로 고려할 때, 검색 속도가 가장 빠르고 공간 활용도 준수한 First-fit(최초 적합)이 일반적으로 성능이 가장 좋다

---

### OS-087
Thrashing 이란 무엇인가요? 🚨

1. 개념
- 메모리에 로드된 프로세스가 너무 많아서 프로세스가 실행되는 시간보다 페이지 교체(Swapping)에 소요되는 시간이 더 많아지는 현상
- 이로 인해 CPU 이용률이 급격히 떨어지고 시스템이 멈춘 듯한 상태가 된다.

- 발생 메커니즘
  - 멀티 프로그래밍 수준이 높아짐, 즉 동시에 실행하는 프로세스 증가
  - 각 프로세스 당 할당되는 물리 메모리(Frame)가 부족
  - 빈번한 page fault가 발생 -> 디스크 I/O(Swapping) 증가
  - CPU는 I/O 작업을 기다리며 유휴 상태(idle)가 된다
  - OS는 CPU가 놀고 있다고 판단해서 더 많은 프로세스를 메모리에 투입
  - 메모리 부족이 심화 -> 무한 반복

2. 사례
- OS 레벨의 스레싱과 유사한 현상이 애플리케이션 레벨에서도 발생
- 1) GC 스레싱(Death Spiral)
  - Java heap 메모리가 거의 가득 찼을 때 발생
  - GC가 메모리를 확보하기 위해 지속적으로 실행되나, 확보되는 공간은 극히 미미
  - 결과적으로 CPU는 GC 작업에 100% 소모되고 실제 애플리케이션 트래픽 처리는 0에 수렴
- 2) Swap & GC
  - JVM Heap 크기를 물리 메모리보다 크게 설정하여 OS 레벨에서 스와핑이 발생하는 경우
  - GC는 힙 영역을 순회해야 하는데, 힙이 디스크(Swap) 영역에 있으면 메모리 접근 속도가 아닌 디스크 속도로 동작하게 되어 심각한 Latency가 발생


### 한줄 답변
- 프로세스에 필요한 최소한의 메모리조차 할당되지 않아, CPU가 실행보다 페이지 교체(I/O)에 대부분의 시간을 낭비하여 시스템 성능이 급락하는 현상

---

### OS-088
Thrashing 발생 시, 어떻게 완화할 수 있을까요?

- 가장 본질적인 방법은 프로세스당 프레임을 더 주거나 or 실행 중인 프로세스 수를 줄이는 것
- Thrashing 방지 알고리즘은 지역성(Locality) 원리를 기반으로 한다

1) 워킹 셋 모델
- 프로세스가 특정 시점에 자주 참조하는 페이지들의 집합(Working Set)을 미리 파악하여 메모리에 통째로 올려놓을 수 있을 때만 프로세스를 실행한다
- 공간이 부족하면 아예 실행하지 않는다(swap out)

2) PFF(Page Fault Frequency) 조절
- 페이지 부재율(Page fault rate)의 상한선과 하한선을 설정한다
- 상한선 초과 시 프레임이 부족하므로 추가 할당
- 하한선 미만 시 프레임이 남으므로 회수
- 남는 프레임이 없는데 상한선을 넘으면 해당 프로세스를 통째로 Swap out시켜 다중 프로그래밍 정도를 낮춘다


### 한줄 답변
- 근본적으로는 멀티 프로그래밍 정도를 낮춰 프로세스당 충분한 프레임을 확보해야 하며, 기법으로는 워킹 셋 모델이나 PFF를 사용하여 스레싱 임계치를 조절한다

---

## 📌 가상 메모리

### OS-089
가상 메모리란 무엇인가요?

1. 개념
- Virtual Memory란 물리 메모리(RAM)의 크기 한계를 극복하기 위해 나온 기술이다.
- 프로세스에게는 연속적이고 큰 메모리 공간(가상 주소)을 제공하고, 실제로는 이를 조각내서 물리 메모리와 디스크(Swap)에 나누어 저장하는 방식이다.
- 핵심은 논리 주소와 물리 주소의 분리다. 그리고 이 변환은 하드웨어인 MMU(Memory Management Unit)가 담당한다.

- 무엇이 가짜(가상)인 건가?
  - 프로세스가 보고 있는 주소(번지수)가 가짜인 것
  - 프로세스의 착각(가상 세계): "와! 나는 0번지부터 42억번지(4GB)까지 나 혼자 다 쓴다! 게다가 메모리가 쭉 이어져 있네(연속적)?"
  - 현실(물리 세계): 실제로는 물리 메모리(RAM)의 구석구석에 조각난 채로 박혀 있거나, 심지어는 하드디스크(Swap)에 쫓겨나 있다.
    - 프로세스 A가 쓰는 100번지와 프로세스 B가 쓰는 100번지는 이름만 같지 실제로는 완전히 다른 물리 공간
  - 프로세스가 사용하는 주소 공간이 물리적인 하드웨어와 일치하지 않는 **'논리적인 가짜 주소'**이기 때문에 가상 메모리라고 부르는 것

- 왜 사기를 칠까
  - 만약 가상 메모리가 없다면? (Real Mode)
    - 개발자가 코딩할 때 "내 프로그램은 RAM의 10,532번지에 저장해야지"라고 직접 계산해야 한다
    - 다른 프로그램이 그 자리를 쓰고 있으면 충돌나서 꺼진다
  - 가상 메모리가 있다면?
    - 개발자는 변수 x 선언하면 끝!
    - OS가 알아서 가상주소(0x100)을 주고 실제로는 남는 물리 메모리 아무 곳에나 매핑해준다
    - "어차피 쟤는 실행할 때 전체 공간 중 극히 일부(지역성)만 건드려. 그러니까 4GB 줬다고 거짓말해 놓고, 실제로는 지금 쓰는 페이지만 몰래 물리 메모리에 넣어줘도 절대 안 들켜."

2. 사례
- JVM Heap 설정 (-Xmx):
  - Spring Boot 실행 시 java -Xmx10G 옵션을 줘도, OS는 실행 즉시 물리 메모리 10GB를 할당하지 않는다
  - OS는 10GB만큼의 가상 주소 공간만 예약해두고, 실제 데이터가 쓰이는 시점에 물리 메모리(Page Frame)를 매핑한다(Demand Paging).
  - 덕분에 물리 메모리가 8GB인 장비에서도 가상 메모리 덕분에 10GB 설정을 띄우는 시도가 가능(물론 실제 사용량이 넘어가면 Swap 발생).

- 멀티 테넌트 환경의 격리:
  - 하나의 서버에 여러 개의 Spring 애플리케이션(또는 Docker 컨테이너)을 띄웠을 때, App A가 버그로 자신의 메모리를 덮어써도 App B는 죽지 않는다
  - 가상 메모리 공간이 독립적이기 때문


### 한줄 답변
- 물리 메모리의 크기 한계를 극복하고 프로세스 간 메모리를 완벽히 격리하기 위해, 논리 주소와 물리 주소를 분리하여 매핑하는 메모리 관리 기술
- 가상 메모리는 물리 메모리보다 큰 프로그램을 실행하기 위해 당장 안 쓰는 데이터를 느린 디스크(Swap)에 내려놓는 기술

---

### OS-090
가상 메모리가 가능한 이유가 무엇일까요?

- 가상 메모리가 작동할 수 있는 배경은 참조의 지역성이다.
- 프로세스는 실행되는 동안 모든 코드와 데이터를 균일하게 참조하지 않는다.
- 특정 시간 동안은 특정 메모리 영역만 집중적으로 참조하는 경향
- 이 특성 덕에 운영체제는 프로세스 전체를 물리 메모리에 올리지 않고, 현재 실행에 필요한 일부분만 올려도 문제없이 프로그램을 실행할 수 있다

2. 사례: 넷플릭스 스트리밍
   - 영화 파일(프로세스 전체): 10GB 
   - 내 스마트폰 메모리(RAM): 100MB 
   - 상황: 10GB 영화를 100MB 버퍼로 끊김 없이 본다
   - 이유 (지역성): 우리는 영화를 0분부터 120분까지 **순차적(시간/공간적 지역성)**으로 본다
   - 지금 보고 있는 1분치 영상만 메모리에 있으면 됨. 나머지는 서버(디스크)에 있어도 상관 X
   - 만약 지역성이 없다면?: 1분 봤다가 50분 봤다가 10분 봤다가를 0.1초마다 반복한다면? 계속 로딩(Buffering)만 걸려서 영화를 못 볼 것

- 즉 지역성이 있기에 현재 실행에 필요한 '일부 페이지(Working Set)'만 물리 메모리에 유지해도 성능 저하가 발생하지 않으므로, **나머지 데이터를 디스크에 두는 가상 메모리 기법**이 실효성을 갖게 됨

### 한줄 답변
- 프로세스는 실행 중 특정 부분만 집중적으로 참조하는 '지역성(Locality)'을 띠기 때문에, 전체가 아닌 필요한 일부분만 메모리에 적재하여 실행하는 것이 가능

### 꼬리 질문
- 디스크의 개념이 무엇인지?

---

### OS-091
Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.

1. 개념
- Page fault: 프로그램이 참조하려는 페이지가 물리 메모리(RAM)에 없을 때 발생하는 하드웨어 인터럽트(Trap)이다.
- 핵심 처리 과정
  - 1. 트랩 발생: CPU가 주소를 요청함 -> 데이터가 없어서 MMU가 OS에게 Page Fault Trap을 발생시킴
  - 2. Context Serving: 현재 프로세스의 상태(레지스터)를 저장 -> 프로세스를 Block(Waiting) 상태로 변경(디스크 I/O는 느리기 때문)
  - 3. 유효성 검사: OS는 요청한 주소가 유효한지 확인, 잘못된 접근이면 Segmentation Fault로 프로세스 강제 종료
  - 4. 가용 프레임 확보:
    - 메모리에 빈 공간(Free Frame)이 있다면 바로 사용
    - 빈 공간이 없다면 **페이지 교체 알고리즘**이 작동 -> Victim 페이지를 골라서 디스크로 Swap Out, 쫓아냄
  - 5. Disk I/O(Swap-in): 디스크에서 해당 페이지를 읽어 물리 메모리의 빈 프레임에 적재한다
  - 6. 테이블 업데이트 및 재개: 페이지 테이블의 Valid bit를 1로 설정 -> 저장해뒀던 CPU 레지스터를 복구 -> 중단되었던 명령어를 처음부터 다시 실행


### 한줄 답변
- Page Fault가 발생하면 프로세스를 대기(Block) 상태로 두고, OS가 디스크에서 해당 페이지를 읽어 빈 프레임에 적재(Swap-in)한 뒤 페이지 테이블을 갱신하고 명령어를 재실행

### 꼬리 질문
- 메모리가 꽉 찼을 때 누구를 Swap out 할지 결정하는 페이지 교체 알고리즘의 종류에는 어떤 것들이 있는지?
- Page Fault 처리 중에 Context Switch가 발생하는지?

---

### OS-092
페이지 크기에 대한 Trade-Off를 설명해 주세요.

1. 개념
- 페이지(Page): 가상 메모리를 일정한 크기로 나눈 단위
- 페이지 크기(Page Size): 한 페이지가 차지하는 메모리 용량
- 페이지 단위로 메모리 할당, 스와핑, 캐시, TLB(Translation Lookaside Buffer) 조회 등이 이루어짐

2. 작은 페이지 크기
- 장점
  - 메모리 사용 효율성이 높다(외부 단편화 감소) - 프로세스가 작은 메모리 단위를 필요로 할 때 낭비가 적다
  - 페이지 단위 I/O가 작아서 스왑이나 페이징 오버헤드가 낮음
  - TLB 미스 시 페이징 비용이 상대적으로 적음
- 단점
  - 페이지 테이블의 크기 증가
  - TLB 적중률 저하 가능성 -> 더 자주 메모리 참조 -> 성능 저하

3. 큰 페이지 크기
- 장점
  - 페이지 테이블 엔트리 수 감소 -> 메모리 오버헤드 감소
  - TLB 적중률 증가 -> 메모리 접근 속도 향상
  - 연속적인 메모리 접근에 효율적
- 단점
  - 외부 단편화 증가 - 작은 데이터만 필요해도 큰 페이지 단위를 할당 -> 메모리 낭비
  - 스왑 시 I/O 단위가 커서 페이징 비용 증가
  - 작은 객체를 많이 사용하는 프로그램에서는 오히려 비효율적

### 한줄 답변
- 페이지를 작게 쓰면 메모리 효율적이고 크게 쓰면 접근 속도가 빠르다는 Trade-Off가 존재함
  즉, 메모리 낭비와 접근 속도 사이의 균형을 조절하는 것이 페이지 크기 선택의 핵심

### 꼬리 질문
- 페이지 크기가 작으면 왜 페이지 테이블의 크기가 증가하는가?

---

### OS-093
페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?

- 페이지 크기와 페이지 폴트 빈도는 U자형 곡선과 비슷한 관계를 지닌다
- 페이지 폴트가 감소하는 구간
  - 페이지가 커지면 한 번의 I/O로 더 많은 데이터를 메모리에 적재할 수 있다
  - 공간적 지역성 덕에 방근 읽어온 큰 페이지 안에 내가 곧 참조할 데이터가 포함되어있을 확률이 높다 -> 추가적인 I/O가 줄어든다
- 페이지 폴트가 증가하는 구간
  - 페이지가 지나치게 커지면, 그 페이지 안에 지금 당장 쓰지 않을 불필요한 데이터까지 너무 많이 포함될 가능성이 있다
  - 그래서 물리 메모리에 들어갈 수 있는 페이지의 총 개수(Entry)가 줄어든다
  - 정작 꼭 필요한 다른 페이지(워킹 셋)가 들어올 자리가 없다
  - 결과적으로 필요한 페이지가 없어서 페이지 폴트가 다시 빈번해진다

### 한줄 답변
- 페이지 크기가 커지면 초기에는 공간 지역성 덕분에 페이지 폴트가 감소하지만, 과도하게 커지면 불필요한 데이터까지 적재되어 유효 공간이 줄어들므로 다시 증가할 수 있다
- U자형

### 꼬리 질문
- 페이지 크기가 커졌을 때 발생할 수 있는 부작용은?
- 현대 시스템에서 Huge Page, 즉 대형 페이지를 사용하는 주된 이유는?

---

### OS-094
세그멘테이션 방식을 사용하고 있다면, 가상 메모리를 사용할 수 없을까요?

- 당연히 사용 가능
- 가상 메모리의 핵심은 당장 필요 없는 부분을 디스크(Swap)로 내리는 것
- 페이지(고정 크기) 대신 세그먼트(논리적, 가변 크기)를 통째로 메모리에 올렸다 내렸다 할 수 있다
- 예를 들어, 현재 사용하지 않는 데이터 세그먼트 전체를 디스크로 Swap-out 시킬 수 있다. 이것이 곧 가상 메모리

- 현실 운영체제는?
  - Linux/Windows 등의 현대 OS는 세그멘테이션을 거의 사용하지 않는다
  - 이유는 외부 단편화 관리가 너무 어렵기 때문
  - 가변 크기의 세그먼트를 끼워 맞추는 것보다 블록 맞추기 같은 페이징이 관리하기 훨씬 편리하다


### 한줄 답변
- 사용할 수 있다. 
- 가상 메모리는 기법이 아닌 개념이므로, 고정 크기인 페이지 대신 가변 크기의 세그먼트 단위로 메모리를 교체(Swapping)하는 방식으로도 가상 메모리를 구현할 수 있다

---

### OS-095
세그멘테이션과 페이징의 차이점은 무엇인가요?

| 구분     | 세그멘테이션                           | 페이징(가상메모리의 기법)                    |
|--------|----------------------------------|-----------------------------------|
| 목적     | 프로그램의 논리 구조를 반영해서 메모리 관리         | 메모리 단편화 최소화 및 효율적 물리 메모리 관리       |
| 단위     | 논리적 단위 (코드, 데이터, 스택 등 프로그램 논리 구조) | 고정 크기 블록 (페이지, 일반적으로 4KB)         |
| 크기     | 가변적                              | 고정적                               |
| 주소 구조  | 논리 주소 = 세그먼트 번호 + 오프셋            | 논리 주소 = 페이지 번호 + 페이지 오프셋          |
| 메모리 연속성 | 연속적이어야 함 (각 세그먼트가 연속 메모리 공간 필요)  | 연속적일 필요 없음 (프레임 단위로 물리 메모리에 분산 가능) |
| 단점     | 외부 단편화 발생 가능                     | 내부 단편화 발생 가능 (페이지보다 작은 데이터 잔여 공간) |



### 한줄 답변
- 세그멘테이션은 논리적 관점에서 코드, 데이터 스택 등을 기준으로 메모리를 나누어 관리하는 방식이고
- 페이징은 고정 크기 블록(페이징) 단위로 메모리를 나누어 외부 단편화를 없애고 효율적으로 관리하는 방식이다.
- 세그멘테이션은 논리적 단위라 보호/공유에 유리하지만 외부 단편화가 발생. 페이징은 고정 크기라 관리가 쉽고 외부 단편화가 없지만 내부 단편화가 발생할 수 있다.

---

### OS-096
페이지와 프레임의 차이에 대해 설명해 주세요.

- 페이지(Page): 가상 메모리를 일정 크기(보통 4KB)로 나눈 단위. 프로세스가 사용하는 **논리적 메모리 단위**
  - 즉 가상 메모리 단위
- 프레임(Frame): 실제 물리 메모리를 **같은 크기(페이지와 동일)로 나눈 단위**. **CPU가 접근하는 실제 메모리 공간**
  - 즉 물리 메모리 단위

- 논리 주소(Page 번호 + 오프셋) → 물리 주소(Frame 번호 + 오프셋) 로 변환됨

### 한줄 답변
- 페이지는 가상 메모리 단위, 프레임은 물리 메모리 단위이며, CPU는 페이지를 프레임에 매핑하여 접근한다.

---

### OS-097
내부 단편화와, 외부 단편화에 대해 설명해 주세요.

- 내부 단편화(Internal Fragmentation): 할당된 메모리 블록(페이지, 프레임)이 요청한 메모리보다 커서 남는 공간이 내부에 생기는 현상.
- 외부 단편화(External Fragmentation): 할당·해제 반복으로 사용 가능한 메모리 공간이 흩어져 있어, 총합은 충분하지만 연속된 공간이 없어 요청을 만족시키지 못하는 현상.

### 한줄 답변
- 내부 단편화는 할당 블록 안에 남는 낭비 공간, 외부 단편화는 흩어진 빈 공간 때문에 연속 할당이 불가능한 문제이다.

---

### OS-098
페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.

- 페이지(Page)에서 실제 주소(Frame 주소)를 가져오는 과정 = 주소 변환(Address Translation)
  - 논리 주소(Logical Address) = 페이지 번호(Page Number) + 페이지 오프셋(Page Offset)
  - 물리 주소(Physical Address) = 프레임 번호(Frame Number) + 페이지 오프셋(Page Offset)

- 과정
  - CPU가 메모리에 접근할 때 페이지 번호를 추출
  - 페이지 테이블에서 해당 페이지 번호에 매핑된 프레임 번호를 찾음
  - 페이지 오프셋을 더해서 실제 물리 주소를 계산

- 즉 실제 주소 = 페이지 테이블[페이지 번호] X 페이지 크기 + 페이지 오프셋


### 한줄 답변
- 페이지에서 실제 주소를 가져오는 과정은 페이지 번호를 페이지 테이블로 조회해 프레임 번호를 얻고, **페이지 오프셋**을 더해 물리 주소를 계산하는 방식이다.

---

### OS-099
어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?

- 주소공간의 접근 권한 확인은 운영체제가 **페이지 테이블(Page Table)**과 프로세스 권한 보호 구조를 통해 관리
  - 페이지 테이블에는 각 페이지의 접근 권한 정보가 있음:
    - 읽기(Read)
    - 쓰기(Write)
    - 실행(Execute)
  - CPU는 메모리 접근 시 권한 비트를 확인하여 허용되지 않은 작업이면 Segmentation Fault 또는 Page Fault 발생

- 방법
  - 페이지 테이블 조회 → OS 레벨에서 접근 가능 여부 확인
  - 시도 접근 → 보호 위반 발생 여부로 확인
  - 현대 OS에서는 mprotect()(Linux), VirtualProtect()(Windows) 같은 API로 프로그래머가 권한 확인/변경 가능


### 한줄 답변
- 주소공간이 수정 가능한지는 페이지 테이블의 접근 권한 비트나 OS API(mprotect, VirtualProtect)를 통해 확인할 수 있다.

---

### OS-100
32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?

- 페이지 테이블 크기 계산
  - 주소 공간 크기: 32비트 → 2³² = 4GB
  - 페이지 크기: 1KB = 2¹⁰ bytes
  - 페이지 수 = 주소 공간 / 페이지 크기

### 한줄 답변
- 32비트 주소 공간에서 페이지 크기 1KB일 때, 페이지 테이블 엔트리는 최대 2²²개(약 419만 개) 필요

---

### OS-101
32비트 운영체제는 램을 최대 4G 까지 사용할 수 있습니다. 이 이유를 페이징과 연관 지어서 설명해 주세요.

- 32비트 주소 공간: CPU가 사용할 수 있는 논리 주소가 32비트 -> 최대 주소값 = 2의 32승 -> 4GB
- 페이징의 관점:
  - 가상 주소 공간 = 페이지 번호 + 페이지 오프셋
  - 페이지 번호가 32비트 주소를 결정 -> 페이지 수 = 2의 32승/페이지 크기
  - 32비트 주소로 표현 가능한 물리 주소가 최대 4GB -> OS가 페이징을 통해 이 범위 내에서 프레임에 매핑 가능

- 즉 CPU가 표현할 수 있는 최대 주소 범위 = 32비트 -> 4GB, 페이징은 그 범위 안에서 메모리를 세분화해서 관리


### 한줄 답변
- 32비트 운영체제에서 최대 4GB만 사용할 수 있는 이유는, CPU가 32비트 주소로 페이지를 표현할 수 있어 **가상 주소 공간이 4GB로 제한**되기 때문이다.

---

### OS-102
C/C++ 개발을 하게 되면 Segmentation Fault 라는 에러를 접할 수 있을텐데, 이 에러는 세그멘테이션/페이징과 어떤 관계가 있을까요?

---

## 📌 TLB

### OS-103
TLB는 무엇인가요?

- TLB(Translation Lookaside Buffer): **CPU 내부의 주소 변환 캐시**
- 목적: 가상 주소 → 물리 주소 변환 속도 향상
- 작동 원리:
  - CPU가 가상 주소 접근 시 페이지 번호를 추출
  - MMU가 TLB(버퍼)에서 해당 페이지 번호를 검색
    - Hit → 바로 프레임 번호 반환 → 빠른 메모리 접근
    - Miss → **페이지 테이블 조회** 후 TLB 갱신

- 페이지 테이블 전체를 조회하는 것보다 훨씬 빠름

### 한줄 답변
- TLB는 가상 주소를 물리 주소로 빠르게 변환하기 위해 CPU가 사용하는 주소 변환 캐시이다.

---

### OS-104
TLB를 쓰면 왜 빨라지나요?

- 문제점: 가상 주소 → 물리 주소 변환 시 매번 **페이지 테이블(Page Table) 조회** 필요 → **메모리 접근 속도가 느림**
- 해결책: TLB
  - 소형, 초고속 캐시에 **최근 접근한 페이지 번호와 프레임 번호를 저장** -> 이것도 지역성을 활용한 것
  - CPU가 메모리에 접근할 때 TLB에서 바로 변환 → 메모리 접근 속도 향상

### 한줄 답변
- TLB는 최근 접근한 페이지 매핑을 캐싱해 페이지 테이블 조회를 생략하므로, 메모리 접근 속도를 크게 향상시킨다.

---

### OS-105
MMU가 무엇인가요?

- MMU(Memory Management Unit): CPU 내부 또는 CPU와 메모리 사이에 있는 하드웨어 장치
- 역할
  - 가상주소 -> 물리주소 변환
  - 메모리 보호
  - TLB와 협력해서 주소 변환 속도 향상

### 한줄 답변
- MMU는 가상 주소를 물리 주소로 변환하고 메모리 보호를 수행하는 **CPU 내부 하드웨어 장치**이다.
- TLB랑 협력

---

### OS-106
TLB와 MMU는 어디에 위치해 있나요?

- MMU: CPU 내부 또는 CPU와 메모리 사이
- TLB: MMU 내부에 작은 캐시 형태로 존재
- 즉, CPU → MMU → TLB → 실제 메모리 접근 순서로 동작

### 한줄 답변
- MMU는 CPU 내부 또는 CPU-메모리 사이에 위치하며, TLB는 MMU 내부의 작은 캐시로서 가상→물리 주소 변환을 빠르게 수행한다.

---

### OS-107
코어가 여러개라면, TLB는 어떻게 동기화할 수 있을까요?

- 문제: 멀티코어 CPU에서 각 코어마다 독립적인 TLB를 가지고 있을 수 있음 → 주소 매핑 정보가 달라서 동기화 필요
- 해결:
- 1. TLB Shootdown
  - 한 코어에서 페이지 테이블 변경 시, 다른 코어의 TLB에 있는 해당 엔트리를 무효화
  - OS가 인터럽트를 통해 다른 코어에 알림 -> 동기화
- 2. 공유 TLB
  - 여러 코어가 접근하는 상위 레벨 TLB를 공유 -> 별도 동기화 비용 감소
- 3. 하드웨어 지원
  - 최신 CPU는 TLB 동기화를 하드웨어에서 지원 -> 자동 동기화

### 한줄 답변
- 멀티코어에서 TLB 동기화는 OS의 Shootdown, 공유 TLB, 또는 하드웨어 지원을 통해 가상→물리 주소 매핑의 일관성을 유지함으로써 이루어진다.

---

### OS-108
TLB 관점에서, Context Switching 발생 시 어떤 변화가 발생하는지 설명해 주세요.

- 문제: Context Switching → CPU가 한 프로세스(P0)에서 다른 프로세스(P1)로 전환
  - 각 프로세스는 독립적인 가상 주소 공간을 가짐
  - TLB에는 이전 프로세스 P0의 페이지 매핑 정보가 존재
- 해결
  - 1) TLB Flush(무효화)
    - 대부분의 OS는 Context Switch 시 TLB 엔트리를 초기화
    - 새로운 프로세스 P1 접근 시 TLB Miss -> 페이지 테이블 조회 후 TLB 갱신
  - 2) ASID(Address Space Identifier) 활용
    - 일부 현대 CPU는 TLB 엔트리에 ASID를 저장 -> 서로 다른 프로세스의 엔트리를 구분
    - TLB Flush 없이도 다른 프로세스 매핑을 보호 가능 -> 성능 향상

- 핵심: Context Switch 시 TLB의 가상 -> 물리 매핑이 이전 프로세스와 섞이지 않도록 조치할 필요

### 한줄 답변
- Context Switch 시 TLB는 이전 프로세스 매핑과 충돌하지 않도록 Flush하거나 ASID를 사용하여 가상→물리 주소 매핑을 안전하게 관리한다.

---

### OS-109
동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.

- 문제: 멀티코어 환경에서 여러 스레드가 동시에 같은 메모리에 접근하면 Race Condition 발생 가능
- 하드웨어적 동기화 기법: CPU 레벨에서 원자적 연산 제공

1. TSL(Test and Set, Test and Set Lock)
- 메모리 값 읽기 + 특정 값으로 변경을 한 번의 연산으로 수행
- 스핀락 구현에 사용

2. CAS(Compare-and-Swap)
- 기대값과 메모리 값이 같으면 새로운 값으로 변경, 아니면 실패 → 반복 시도
- Java의 AtomicInteger 등에서 사용 (java.util.concurrent 패키지)

3. Load-Linked / Store-Conditional (LL/SC)
- LL: 메모리 값을 읽음
- SC: 값이 LL 이후 바뀌지 않았다면 저장 성공, 바뀌면 실패 → 반복

### 한줄 답변
- 하드웨어적 동기화는 CAS, Test-and-Set, LL/SC 같은 원자적 연산을 통해 락 없이 스레드 안전성을 구현하는 방법이다.

---

### OS-110
volatile 키워드는 어떤 의미가 있나요?

- **캐시 패싱**
- volatile 키워드: Java에서 **변수의 메인 메모리 접근을 보장**하고, 컴파일러/CPU 최적화로 인한 재정렬(reordering) 방지 -> 가시성 보장
  - 변수 앞에 volatile을 붙이면 (volatile int count), CPU에게 경고
    - "이 변수는 중요한 놈이니까, 네 캐시(개인 노트) 믿지 마. 읽을 때도 메모리(칠판)에서 바로 읽어오고, 쓸 때도 캐시 거치지 말고 즉시 메모리(칠판)에 꽂아 넣어."
- 특징
  - 1. 가시성 보장: 한 스레드에서 변경한 값이 다른 스레드에서 즉시 보임
  - 2. 원자적 연산 보장 X: 단일 읽기/쓰기 연산은 안전하지만, 증가 연산 같은 복합 연산은 안전하지 않음
  - 3. 메모리 배리어 역할: CPU가 명령 재정렬을 하지 못하게 하여 순서 보장

### 한줄 답변
- volatile은 CPU 캐시를 거치지 않고 메인 메모리에 직접 읽고 쓰도록 강제하여, 한 스레드의 변경 사항을 다른 스레드가 즉시 볼 수 있게(가시성) 해준다.
- 단, 여러 스레드가 동시에 쓰기를 시도하는 경우의 충돌(원자성 문제)은 막을 수 없다.

---

### OS-111
싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요?

- 캐시 일관성 프로토콜 (MESI 프로토콜): "서로 감시하고 소문내기 (Snooping)", 즉 변경 -> 알림 -> 동기화를 하드웨어 단에서 자동으로
- 동시 write하는 경우: 전체 통로는 놔두고, 딱 이 변수가 들어있는 캐시 라인만 아무도 못 건드리게 lock
- 메모리 배리어: 성능을 위해 CPU는 명령어 순서를 마음대로 바꿀 때가 있다(재정렬) -> 이를 막기 -> 순차 실행 보장 = volatile이 이것 해줌

### 한줄 답변
- 멀티코어 환경에서는 캐시 일관성 프로토콜, 메모리 배리어, 락/원자적 연산을 통해 스레드 간 동기화가 이루어진다.

---

## 📌 페이지 교체 알고리즘

### OS-112
페이지 교체 알고리즘에 대해 설명해 주세요.

### 한줄 답변
- 페이지 교체 알고리즘은 가상 메모리에서 프레임이 가득 찼을 때, 어떤 페이지를 내보낼지 결정해 페이지 부재를 최소화하는 정책이다.

---

### OS-113
LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?

### 한줄 답변
- LRU 알고리즘은 최근에 사용되지 않은 페이지가 앞으로도 사용될 가능성이 낮다는 시간 지역성 특성을 이용한 페이지 교체 정책이다.

---

### OS-114
LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?

- LRU 구현 목표: 가장 오래 사용되지 않은 페이지를 빠르게 찾고 교체
- 대표 구현 방법:
  - 1) **스택/링크드 리스트 방식**
    - 페이지 참조 시 해당 페이지를 리스트 상단으로 이동
    - 하단 페이지가 가장 오래 사용되지 않은 페이지 → 교체
    - 시간 복잡도: O(1) ~ O(n), 자료구조 최적화 필요 (예: LinkedHashMap)
  - 2) **카운터/타임스탬프 방식**
    - 페이지마다 마지막 참조 시간 저장
    - 페이지 교체 시 최소 시간값 가진 페이지 선택
    - 하드웨어 구현에서는 참조 비트와 주기적 스캔 사용 → LRU 근사
  - 3) 하드웨어 지원 LRU
    - 최근 CPU 캐시 라인 교체 정책에서 사용
    - 참조 비트 기반 Clock 알고리즘으로 LRU 근사 구현

### 한줄 답변
- LRU는 링크드 리스트나 카운터/타임스탬프를 활용해 가장 오래 사용되지 않은 페이지를 찾아 교체하는 방식으로 구현할 수 있다.

---

### OS-115
LRU 알고리즘의 단점을 설명해 주세요. 이를 해결할 수 있는 대안에 대해서도 설명해 주세요.

- LRU(Least Recently Used) 알고리즘의 단점
  - 높은 구현 비용: 완전한 LRU 구현 시, 페이지 참조마다 리스트 이동 또는 타임스탬프 갱신 필요 → CPU 오버헤드
  - 캐시 스캔 비용: 카운터 기반 구현 시 페이지 교체 시 모든 페이지 비교 필요 → 비용 증가
  - Thrashing에 취약: 순차적 스캔이나 반복적인 참조 패턴에서 효율이 낮을 수 있음
  - 하드웨어 지원 어려움: 모든 페이지 참조를 기록하는 것은 메모리/하드웨어 부담 → 완전 LRU 구현 불가

- 대안 알고리즘
  - Clock 알고리즘(Second Chance)
  - LFU: 사용 빈도가 낮은 페이지 교체 -> 장기적 참조 패턴 반영

### 한줄 답변
- LRU는 구현 비용과 Thrashing 문제, 하드웨어 부담이 단점이며, Clock, LFU, ARC 같은 근사 알고리즘으로 이를 해결할 수 있다.

---

## 📌 파일 시스템 및 I/O

### OS-116
File Descriptor와, File System에 에 대해 설명해 주세요.

### 한줄 답변
- File Descriptor는 프로세스가 열린 파일을 추상화해 식별하는 핸들이고, File System은 저장 장치에서 데이터를 구조화·관리하는 시스템이다.

---

### OS-117
I-Node가 무엇인가요?

- I-Node(Inode): 유닉스/리눅스 계열 파일 시스템에서 파일 메타데이터를 저장하는 자료구조

### 한줄 답변
- I-Node는 파일의 메타데이터와 데이터 블록 위치를 관리하는 자료구조로, 파일 이름과는 별도로 **OS가 파일 상태를 추적**한다.

---

### OS-118
프로그래밍 언어 상에서 제공하는 파일 관련 함수 (Python - open(), Java - BufferedReader/Writer 등)은, 파일을 어떤 방식으로 읽어들이나요?

- 프로그래밍 언어 파일 I/O의 기본 원리:
  - 애플리케이션에서 제공하는 파일 관련 함수는 OS의 File System과 File Descriptor(FD)를 통해 파일에 접근
  - OS는 I-Node에서 메타데이터와 데이터 블록 위치를 확인
  - 실제 데이터는 버퍼(Buffer) 단위로 메모리에 읽어들여 애플리케이션에 전달
  - 읽기/쓰기 속도 향상을 위해 버퍼링(Buffering), 시퀀셜 접근 등을 활용

- 핵심: 언어 수준 함수는 OS와 File System을 추상화하고, 내부적으로 버퍼를 사용해 효율적 I/O를 수행

- Java의 BufferedReader/Writer
  - `BufferedReader br = new BufferedReader(new FileReader("example.txt"));
    String line;
    while ((line = br.readLine()) != null) {
    System.out.println(line);
    }
    br.close();`
  - FileReader → FD 생성, OS I-Node 확인
  - BufferedReader → 내부 버퍼로 데이터를 여러 줄 단위 읽기
  - 애플리케이션에 라인 단위 제공 → I/O 호출 최소화
- 즉, 언어 레벨 API는 파일 블록 단위로 읽고 버퍼를 통해 효율적 제공


### 한줄 답변
- 프로그래밍 언어의 파일 I/O 함수는 OS의 File Descriptor와 File System을 통해 파일을 블록 단위로 읽고, 버퍼링을 활용해 효율적으로 데이터를 제공한다.

---

### OS-119
동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.

- 동기 vs 비동기 (결과를 누가 신경 쓰나?)
  - 동기 (Sync): 호출자(Caller)가 작업의 결과를 직접 챙긴다.
    - 함수 A가 함수 B를 호출하고, B가 끝날 때까지 A가 결과를 기다리거나(블로킹), 계속 물어본다(논블로킹-폴링).
  - 비동기 (Async): 호출자(Caller)는 결과를 신경 끄고 자기 할 일을 한다.
    - 결과 처리는 콜백 함수나 Future를 통해 **"다 되면 알아서 알려줘"** 라고 맡긴다

- 블로킹 vs 논블로킹 (제어권을 언제 주나?) - 호출된 함수가 즉시 리턴하느냐 아니냐
  - 블로킹 (Blocking): 호출된 함수가 "나 끝날 때까지 너 아무것도 하지 마" 하고 제어권을 꽉 쥐고 안 놔줌
  - 논블로킹 (Non-blocking): 호출된 함수가 "일단 나 바쁘니까 이거(확인증) 먼저 받고 너 할 일 해" 하고 제어권을 즉시 반환한다.

[정리]
- 동기/비동기: 작업의 완료 여부를 누가/언제 처리하는지에 대한 관점 (순서와 결과)
- 블로킹/논블로킹: 호출된 함수가 제어권(Return)을 바로 주는지에 대한 관점 (대기 여부)


### 한줄 답변
- 동기/비동기는 작업 결과 처리의 타이밍(순서) 문제이고, 블로킹/논블로킹은 제어권 반환(대기) 문제이며, 이 조합에 따라 시스템 효율이 달라진다.

### 꼬리 질문
- 비동기 + 논블로킹이 가장 효율적인 이유는?

---

### OS-120
그렇다면, 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?

- 동기이면서 논블로킹 : **계속 물어보기 (Polling)**
  - 동기(Sync): 나(함수 호출자)는 결과가 꼭 필요해서 계속 신경 씀.
  - 논블로킹(Non-Blocking): 상대방(함수)은 "아직 안 됐어"라고 바로 반환함.
  - 동작:
    - 나: "다 됐어?" → 쟤: "아니(즉시 리턴)"
    - 나: (다른 일 좀 하다가) "다 됐어?" → 쟤: "아니(즉시 리턴)"
    - 나: "다 됐어?" → 쟤: "어, 여기 결과(데이터 리턴)"
  - 결론: 이게 바로 **폴링**


- 비동기이면서 블로킹 : 의도치 않은 멍때리기 (Anti-pattern)
  - 상황:
    - 비동기(Async): "다 되면 알려줘" 하고 콜백을 넘김
    - 블로킹(Blocking): 근데 그래놓고 결과 올 때까지 아무것도 안 하고 멍하니 서 있음.
  - 동작:
    - 나: "이거 처리해줘(Async 호출)."
    - 나: (갑자기 불안함) "근데 결과 나올 때까지 나 아무것도 안 할래(Future.get() 호출로 멈춤)."
  - 결론: 안티패턴
    - 개발자가 비동기 코드를 짜놓고 await나 .get()으로 억지로 흐름을 막을 때 발생

### 한줄 답변
- 동기+논블로킹은 완료 여부를 계속 확인하는 '폴링(Polling)' 방식이고, 비동기+블로킹은 비동기 작업의 결과를 억지로 기다리는(Future.get) 형태로 주로 나타난다.

---

### OS-121
I/O 멀티플렉싱에 대해 설명해 주세요.

- 목적: 블로킹 I/O에서 스레드 수를 줄여 효율적 자원 활용
- 동작 방식:
  - 프로세스가 여러 I/O 채널을 모니터링 리스트에 등록 
  - OS가 **readiness(읽기/쓰기 가능 여부)**를 감시 
  - 준비된 채널만 처리 → 나머지 채널은 대기

### 한줄 답변
- I/O 멀티플렉싱은 하나의 스레드가 여러 I/O 채널의 준비 상태를 감시하여, 효율적이고 **논블로킹** 방식으로 처리하는 기법이다.

---

### OS-122
논블로킹 I/O를 수행한다고 하면, 그 결과를 어떻게 수신할 수 있나요?

1. 폴링: 스레드가 반복적으로 I/O 준비 여부 확인 -> CPU 부담 높음
2. 콜백/이벤트: I/O 완료 시 OS 또는 라이브러리가 미리 등록된 콜백 함수 호출
3. Future/Promise: 비동기 작업의 완료 상태와 결과를 나중에 조회
4. Selector/이벤트 루프: NIO, Netty, WebFlux 처럼 단일 스레드에서 이벤트 발생 시 처리

### 한줄 답변
- 논블로킹 I/O는 스레드를 블록하지 않고, 콜백, Future, 이벤트 루프 등을 통해 비동기적으로 결과를 수신한다.

